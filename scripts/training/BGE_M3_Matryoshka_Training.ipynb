{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGE-M3 Matryoshka Fine-tuning + ONNX INT8\n",
    "\n",
    "Fine-tune `BAAI/bge-m3` with **MatryoshkaLoss** for truncatable embeddings `[1024, 768, 512, 256]`, then export to **ONNX INT8**.\n",
    "\n",
    "**Requirements:** A100 GPU (Colab Pro), ~1.5-2h training time.\n",
    "\n",
    "**Output:** Model pushed to HuggingFace Hub.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers>=3.3.0 datasets>=3.0.0 accelerate>=1.0.0 optimum[onnxruntime]>=1.22.0 onnxruntime>=1.19.0 huggingface_hub>=0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os, random\n",
    "import numpy as np\n",
    "\n",
    "# Clear any leftover GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION - edit these values\n",
    "# ============================================================\n",
    "\n",
    "BASE_MODEL = \"BAAI/bge-m3\"\n",
    "HF_REPO_ID = \"tss-deposium/bge-m3-matryoshka-1024d\"\n",
    "MATRYOSHKA_DIMS = [1024, 768, 512, 256]\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32           # A100 80GB handles 32 easily\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "ALLNLI_MAX_SAMPLES = 0    # 0 = all (~560k)\n",
    "NQ_MAX_SAMPLES = 0        # 0 = all (~100k)\n",
    "GOOAQ_MAX_SAMPLES = 50000\n",
    "\n",
    "OUTPUT_DIR = \"./bge-m3-matryoshka-deposium\"\n",
    "ONNX_OUTPUT_DIR = \"./bge-m3-matryoshka-onnx-int8\"\n",
    "\n",
    "# HF Token\n",
    "HF_TOKEN = \"\"\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "    print(f\"HF_TOKEN loaded from Colab secrets\")\n",
    "except Exception:\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"WARNING: No HF_TOKEN. Set it in Colab: Settings > Secrets > HF_TOKEN\")\n",
    "    print(\"The model will still train, but won't push to Hub automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "all_pairs = []\n",
    "\n",
    "# --- all-nli (~560k NLI pairs) ---\n",
    "print(\"Loading sentence-transformers/all-nli...\")\n",
    "try:\n",
    "    nli = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train\")\n",
    "    for row in nli:\n",
    "        all_pairs.append({\"anchor\": row[\"anchor\"], \"positive\": row[\"positive\"]})\n",
    "        if ALLNLI_MAX_SAMPLES and len(all_pairs) >= ALLNLI_MAX_SAMPLES:\n",
    "            break\n",
    "    print(f\"  +{len(all_pairs)} pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping: {e}\")\n",
    "\n",
    "nli_count = len(all_pairs)\n",
    "\n",
    "# --- natural-questions (~100k Q&A) ---\n",
    "print(\"Loading sentence-transformers/natural-questions...\")\n",
    "try:\n",
    "    nq = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "    before = len(all_pairs)\n",
    "    for row in nq:\n",
    "        all_pairs.append({\"anchor\": row[\"query\"], \"positive\": row[\"answer\"]})\n",
    "        if NQ_MAX_SAMPLES and (len(all_pairs) - before) >= NQ_MAX_SAMPLES:\n",
    "            break\n",
    "    print(f\"  +{len(all_pairs) - before} pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping: {e}\")\n",
    "\n",
    "nq_count = len(all_pairs) - nli_count\n",
    "\n",
    "# --- gooaq (50k from 3M+ Q&A) ---\n",
    "print(f\"Loading sentence-transformers/gooaq (max {GOOAQ_MAX_SAMPLES})...\")\n",
    "try:\n",
    "    gooaq = load_dataset(\"sentence-transformers/gooaq\", split=\"train\", streaming=True)\n",
    "    before = len(all_pairs)\n",
    "    for row in gooaq:\n",
    "        all_pairs.append({\"anchor\": row[\"question\"], \"positive\": row[\"answer\"]})\n",
    "        if (len(all_pairs) - before) >= GOOAQ_MAX_SAMPLES:\n",
    "            break\n",
    "    print(f\"  +{len(all_pairs) - before} pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipping: {e}\")\n",
    "\n",
    "gooaq_count = len(all_pairs) - nli_count - nq_count\n",
    "\n",
    "# Shuffle and split\n",
    "random.seed(42)\n",
    "random.shuffle(all_pairs)\n",
    "split_idx = int(len(all_pairs) * 0.95)\n",
    "train_dataset = Dataset.from_list(all_pairs[:split_idx])\n",
    "eval_dataset = Dataset.from_list(all_pairs[split_idx:])\n",
    "\n",
    "print(f\"\\nTotal: {len(all_pairs)} pairs (nli: {nli_count}, nq: {nq_count}, gooaq: {gooaq_count})\")\n",
    "print(f\"Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")\n",
    "\n",
    "# Free raw data\n",
    "del all_pairs, nli, nq\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-tune with MatryoshkaLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU before loading model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading {BASE_MODEL}...\")\n",
    "model = SentenceTransformer(BASE_MODEL)\n",
    "print(f\"Loaded. Embedding dim: {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Loss\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model,\n",
    "    MultipleNegativesRankingLoss(model),\n",
    "    matryoshka_dims=MATRYOSHKA_DIMS,\n",
    "    matryoshka_weights=[1, 1, 1, 1],\n",
    ")\n",
    "\n",
    "# Training\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=train_loss,\n",
    ")\n",
    "\n",
    "print(f\"\\nConfig: batch={BATCH_SIZE}, epochs={NUM_EPOCHS}, lr={LEARNING_RATE}\")\n",
    "print(f\"Steps per epoch: {len(train_dataset) // BATCH_SIZE}\")\n",
    "print(f\"Total steps: {(len(train_dataset) // BATCH_SIZE) * NUM_EPOCHS}\")\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Matryoshka Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Comment resilier un contrat d'assurance ?\",\n",
    "    \"La resiliation d'un contrat peut etre effectuee par lettre recommandee.\",\n",
    "    \"Les previsions meteo annoncent de la pluie demain.\",\n",
    "    \"How to cancel an insurance contract?\",\n",
    "    \"El contrato puede ser rescindido mediante carta certificada.\",\n",
    "]\n",
    "\n",
    "embeddings = model.encode(test_sentences, normalize_embeddings=True)\n",
    "\n",
    "print(f\"{'Dim':>6} | {'Sim(q,doc)':>10} | {'Sim(q,noise)':>12} | {'Delta':>8} | {'Status':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dim in MATRYOSHKA_DIMS:\n",
    "    embs = embeddings[:, :dim]\n",
    "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "    embs = embs / norms\n",
    "    sim_ok = np.dot(embs[0], embs[1])\n",
    "    sim_bad = np.dot(embs[0], embs[2])\n",
    "    delta = sim_ok - sim_bad\n",
    "    print(f\"{dim:>6} | {sim_ok:>10.4f} | {sim_bad:>12.4f} | {delta:>8.4f} | {'OK' if delta > 0.1 else 'WARN':>8}\")\n",
    "\n",
    "print(\"\\nCross-lingual (FR query vs):\")\n",
    "for lang, idx in [(\"FR doc\", 1), (\"EN query\", 3), (\"ES doc\", 4)]:\n",
    "    for dim in [1024, 256]:\n",
    "        embs = embeddings[:, :dim]\n",
    "        norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "        embs = embs / norms\n",
    "        print(f\"  {lang} @ {dim}D: {np.dot(embs[0], embs[idx]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export ONNX INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import export_dynamic_quantized_onnx_model\n",
    "    print(\"Exporting ONNX INT8 (sentence-transformers native)...\")\n",
    "    export_dynamic_quantized_onnx_model(\n",
    "        model,\n",
    "        quantization_config=\"avx512_vnni\",\n",
    "        model_name_or_path=ONNX_OUTPUT_DIR,\n",
    "    )\n",
    "    print(f\"Saved to {ONNX_OUTPUT_DIR}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Native export failed: {e}\")\n",
    "    print(\"Falling back to HuggingFace Optimum...\")\n",
    "\n",
    "    from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTQuantizer\n",
    "    from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "    import shutil\n",
    "\n",
    "    ort_model = ORTModelForFeatureExtraction.from_pretrained(OUTPUT_DIR, export=True)\n",
    "    ort_model.save_pretrained(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\")\n",
    "\n",
    "    quantizer = ORTQuantizer.from_pretrained(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\")\n",
    "    qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "    quantizer.quantize(save_dir=ONNX_OUTPUT_DIR, quantization_config=qconfig)\n",
    "\n",
    "    for f in Path(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\").glob(\"*.json\"):\n",
    "        if \"model\" not in f.name.lower() or f.name == \"config.json\":\n",
    "            shutil.copy2(f, ONNX_OUTPUT_DIR)\n",
    "    shutil.rmtree(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\", ignore_errors=True)\n",
    "    print(f\"Saved to {ONNX_OUTPUT_DIR}\")\n",
    "\n",
    "total_size = sum(f.stat().st_size for f in Path(ONNX_OUTPUT_DIR).rglob(\"*\") if f.is_file())\n",
    "print(f\"ONNX model size: {total_size / 1e6:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Push to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from huggingface_hub import HfApi\n\nif not HF_TOKEN:\n    print(\"No HF_TOKEN. To push manually:\")\n    print(f\"  !huggingface-cli login\")\n    print(f\"  !huggingface-cli upload {HF_REPO_ID} {OUTPUT_DIR}\")\n    print(f\"  !huggingface-cli upload {HF_REPO_ID}-onnx-int8 {ONNX_OUTPUT_DIR}\")\nelse:\n    # Pass token directly to HfApi (login() has auth issues on Colab)\n    api = HfApi(token=HF_TOKEN)\n\n    print(f\"Pushing PyTorch model to {HF_REPO_ID}...\")\n    api.create_repo(HF_REPO_ID, exist_ok=True, private=False)\n    api.upload_folder(folder_path=OUTPUT_DIR, repo_id=HF_REPO_ID,\n                      commit_message=\"BGE-M3 Matryoshka [1024, 768, 512, 256]\")\n    print(f\"Done: https://huggingface.co/{HF_REPO_ID}\")\n\n    onnx_repo = f\"{HF_REPO_ID}-onnx-int8\"\n    print(f\"\\nPushing ONNX INT8 to {onnx_repo}...\")\n    api.create_repo(onnx_repo, exist_ok=True, private=False)\n    api.upload_folder(folder_path=ONNX_OUTPUT_DIR, repo_id=onnx_repo,\n                      commit_message=\"BGE-M3 Matryoshka ONNX INT8 [1024, 768, 512, 256]\")\n    print(f\"Done: https://huggingface.co/{onnx_repo}\")"
  }
 ]
}