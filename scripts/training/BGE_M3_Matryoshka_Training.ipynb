{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BGE-M3 Matryoshka Fine-tuning + ONNX INT8\n",
        "\n",
        "Fine-tune `BAAI/bge-m3` with **MatryoshkaLoss** for truncatable embeddings `[1024, 768, 512, 256]`, then export to **ONNX INT8**.\n",
        "\n",
        "**Requirements:** T4 GPU (free Colab), ~4-6h training time.\n",
        "\n",
        "**Output:** Model pushed to `tss-deposium/bge-m3-matryoshka-1024d` on HuggingFace Hub.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q sentence-transformers>=3.3.0 datasets>=3.0.0 accelerate>=1.0.0 optimum[onnxruntime]>=1.22.0 onnxruntime>=1.19.0 huggingface_hub>=0.25.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "import os\n",
        "\n",
        "# Set your HF token (Colab: Settings > Secrets > add HF_TOKEN)\n",
        "# Or uncomment and paste directly:\n",
        "# os.environ[\"HF_TOKEN\"] = \"hf_your_token_here\"\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "    print(\"HF_TOKEN loaded from Colab secrets\")\n",
        "except Exception:\n",
        "    HF_TOKEN = os.getenv(\"HF_TOKEN\", \"\")\n",
        "    if HF_TOKEN:\n",
        "        print(\"HF_TOKEN loaded from environment\")\n",
        "    else:\n",
        "        print(\"WARNING: No HF_TOKEN found. Set it above to push to Hub.\")\n",
        "\n",
        "# Model config\n",
        "BASE_MODEL = \"BAAI/bge-m3\"\n",
        "HF_REPO_ID = \"tss-deposium/bge-m3-matryoshka-1024d\"\n",
        "MATRYOSHKA_DIMS = [1024, 768, 512, 256]\n",
        "\n",
        "# Training config\n",
        "NUM_EPOCHS = 4\n",
        "BATCH_SIZE = 16  # T4 16GB handles this fine, reduce to 8 if OOM\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# Dataset config\n",
        "MIRACL_LANGUAGES = [\"fr\", \"en\", \"de\", \"es\", \"ar\", \"zh\", \"ja\", \"ru\"]\n",
        "MIRACL_MAX_SAMPLES = 50_000\n",
        "MMARCO_MAX_SAMPLES = 20_000\n",
        "\n",
        "# Output\n",
        "OUTPUT_DIR = \"./bge-m3-matryoshka-deposium\"\n",
        "ONNX_OUTPUT_DIR = \"./bge-m3-matryoshka-onnx-int8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset (MIRACL + mMARCO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import random\n",
        "\n",
        "all_pairs = []\n",
        "\n",
        "# --- MIRACL (multilingual retrieval) ---\n",
        "for lang in MIRACL_LANGUAGES:\n",
        "    try:\n",
        "        print(f\"Loading MIRACL/{lang}...\")\n",
        "        ds = load_dataset(\"miracl/miracl\", lang, split=\"train\", trust_remote_code=True)\n",
        "        count_before = len(all_pairs)\n",
        "        for row in ds:\n",
        "            query = row.get(\"query\", \"\")\n",
        "            for pos in row.get(\"positive_passages\", []):\n",
        "                text = pos.get(\"text\", \"\")\n",
        "                if query and text:\n",
        "                    all_pairs.append({\"anchor\": query, \"positive\": text})\n",
        "            if len(all_pairs) >= MIRACL_MAX_SAMPLES:\n",
        "                break\n",
        "        print(f\"  +{len(all_pairs) - count_before} pairs (total: {len(all_pairs)})\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Skipping {lang}: {e}\")\n",
        "    if len(all_pairs) >= MIRACL_MAX_SAMPLES:\n",
        "        break\n",
        "\n",
        "miracl_count = len(all_pairs)\n",
        "print(f\"\\nMIRACL total: {miracl_count} pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- mMARCO (multilingual MS MARCO, French) ---\n",
        "try:\n",
        "    print(\"Loading mMARCO/french (streaming)...\")\n",
        "    ds = load_dataset(\"unicamp-dl/mmarco\", \"french\", split=\"train\", streaming=True)\n",
        "    mmarco_count = 0\n",
        "    for row in ds:\n",
        "        query = row.get(\"query\", \"\")\n",
        "        positive = row.get(\"positive\", \"\")\n",
        "        if query and positive:\n",
        "            all_pairs.append({\"anchor\": query, \"positive\": positive})\n",
        "            mmarco_count += 1\n",
        "        if mmarco_count >= MMARCO_MAX_SAMPLES:\n",
        "            break\n",
        "    print(f\"  +{mmarco_count} pairs from mMARCO\")\n",
        "except Exception as e:\n",
        "    print(f\"  Skipping mMARCO: {e}\")\n",
        "\n",
        "print(f\"\\nTotal dataset: {len(all_pairs)} pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle and split\n",
        "random.seed(42)\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "split_idx = int(len(all_pairs) * 0.95)\n",
        "train_dataset = Dataset.from_list(all_pairs[:split_idx])\n",
        "eval_dataset = Dataset.from_list(all_pairs[split_idx:])\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")\n",
        "print(f\"\\nSample: {train_dataset[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fine-tune with MatryoshkaLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    SentenceTransformerTrainer,\n",
        "    SentenceTransformerTrainingArguments,\n",
        ")\n",
        "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
        "\n",
        "# Load base model\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "model = SentenceTransformer(BASE_MODEL)\n",
        "print(f\"Loaded. Embedding dim: {model.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "# Matryoshka + MNRL loss\n",
        "inner_loss = MultipleNegativesRankingLoss(model)\n",
        "train_loss = MatryoshkaLoss(\n",
        "    model,\n",
        "    inner_loss,\n",
        "    matryoshka_dims=MATRYOSHKA_DIMS,\n",
        "    matryoshka_weights=[1, 1, 1, 1],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=train_loss,\n",
        ")\n",
        "\n",
        "# Train!\n",
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Validate Matryoshka Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_sentences = [\n",
        "    \"Comment resilier un contrat d'assurance ?\",\n",
        "    \"La resiliation d'un contrat peut etre effectuee par lettre recommandee avec accuse de reception.\",\n",
        "    \"Les previsions meteo annoncent de la pluie demain.\",\n",
        "    \"How to cancel an insurance contract?\",\n",
        "    \"El contrato puede ser rescindido mediante carta certificada.\",\n",
        "]\n",
        "\n",
        "embeddings = model.encode(test_sentences, normalize_embeddings=True)\n",
        "\n",
        "print(f\"{'Dim':>6} | {'Sim(q,doc)':>10} | {'Sim(q,noise)':>12} | {'Delta':>8} | {'Status':>8}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for dim in MATRYOSHKA_DIMS:\n",
        "    embs = embeddings[:, :dim]\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    embs = embs / norms\n",
        "    sim_relevant = np.dot(embs[0], embs[1])\n",
        "    sim_noise = np.dot(embs[0], embs[2])\n",
        "    delta = sim_relevant - sim_noise\n",
        "    status = \"OK\" if delta > 0.1 else \"WARN\"\n",
        "    print(f\"{dim:>6} | {sim_relevant:>10.4f} | {sim_noise:>12.4f} | {delta:>8.4f} | {status:>8}\")\n",
        "\n",
        "print(\"\\nCross-lingual (FR query vs):\")\n",
        "for lang, idx in [(\"FR doc\", 1), (\"EN query\", 3), (\"ES doc\", 4)]:\n",
        "    for dim in [1024, 256]:\n",
        "        embs = embeddings[:, :dim]\n",
        "        norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "        embs = embs / norms\n",
        "        print(f\"  {lang} @ {dim}D: {np.dot(embs[0], embs[idx]):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Export ONNX INT8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try sentence-transformers native export first\n",
        "try:\n",
        "    from sentence_transformers import export_dynamic_quantized_onnx_model\n",
        "\n",
        "    print(\"Exporting with sentence-transformers native method...\")\n",
        "    export_dynamic_quantized_onnx_model(\n",
        "        model,\n",
        "        quantization_config=\"avx512_vnni\",\n",
        "        model_name_or_path=ONNX_OUTPUT_DIR,\n",
        "    )\n",
        "    print(f\"ONNX INT8 saved to {ONNX_OUTPUT_DIR}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Native export failed: {e}\")\n",
        "    print(\"Falling back to HuggingFace Optimum...\")\n",
        "\n",
        "    from optimum.onnxruntime import ORTModelForFeatureExtraction, ORTQuantizer\n",
        "    from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "    import shutil\n",
        "\n",
        "    # Export FP32\n",
        "    ort_model = ORTModelForFeatureExtraction.from_pretrained(OUTPUT_DIR, export=True)\n",
        "    ort_model.save_pretrained(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\")\n",
        "\n",
        "    # Quantize to INT8\n",
        "    quantizer = ORTQuantizer.from_pretrained(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\")\n",
        "    qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
        "    quantizer.quantize(save_dir=ONNX_OUTPUT_DIR, quantization_config=qconfig)\n",
        "\n",
        "    # Copy tokenizer files\n",
        "    from pathlib import Path\n",
        "    for f in Path(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\").glob(\"*.json\"):\n",
        "        if \"model\" not in f.name.lower() or f.name == \"config.json\":\n",
        "            shutil.copy2(f, ONNX_OUTPUT_DIR)\n",
        "    shutil.rmtree(f\"{ONNX_OUTPUT_DIR}-fp32-tmp\", ignore_errors=True)\n",
        "\n",
        "    print(f\"ONNX INT8 saved to {ONNX_OUTPUT_DIR}\")\n",
        "\n",
        "# Show model size\n",
        "import os\n",
        "total_size = sum(f.stat().st_size for f in Path(ONNX_OUTPUT_DIR).rglob(\"*\") if f.is_file())\n",
        "print(f\"Total ONNX model size: {total_size / 1e6:.0f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Push to HuggingFace Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, login\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"No HF_TOKEN set. Run this to push manually:\")\n",
        "    print(f\"  huggingface-cli login\")\n",
        "    print(f\"  huggingface-cli upload {HF_REPO_ID} {OUTPUT_DIR}\")\n",
        "    print(f\"  huggingface-cli upload {HF_REPO_ID}-onnx-int8 {ONNX_OUTPUT_DIR}\")\n",
        "else:\n",
        "    login(token=HF_TOKEN)\n",
        "    api = HfApi()\n",
        "\n",
        "    # Push PyTorch model\n",
        "    print(f\"Pushing PyTorch model to {HF_REPO_ID}...\")\n",
        "    api.create_repo(HF_REPO_ID, exist_ok=True, private=False)\n",
        "    api.upload_folder(\n",
        "        folder_path=OUTPUT_DIR,\n",
        "        repo_id=HF_REPO_ID,\n",
        "        commit_message=\"BGE-M3 Matryoshka [1024, 768, 512, 256]\",\n",
        "    )\n",
        "    print(f\"Done: https://huggingface.co/{HF_REPO_ID}\")\n",
        "\n",
        "    # Push ONNX INT8 model\n",
        "    onnx_repo = f\"{HF_REPO_ID}-onnx-int8\"\n",
        "    print(f\"\\nPushing ONNX INT8 model to {onnx_repo}...\")\n",
        "    api.create_repo(onnx_repo, exist_ok=True, private=False)\n",
        "    api.upload_folder(\n",
        "        folder_path=ONNX_OUTPUT_DIR,\n",
        "        repo_id=onnx_repo,\n",
        "        commit_message=\"BGE-M3 Matryoshka ONNX INT8 [1024, 768, 512, 256]\",\n",
        "    )\n",
        "    print(f\"Done: https://huggingface.co/{onnx_repo}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done!\n",
        "\n",
        "Models are now on HuggingFace Hub. Next steps:\n",
        "1. Update `model_manager.py` to reference the new model\n",
        "2. Add `truncate_dims` support in `OnnxEmbeddingModel`\n",
        "3. Test in staging with the existing benchmark suite"
      ]
    }
  ]
}
