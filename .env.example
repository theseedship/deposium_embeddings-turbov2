# Performance Optimization Environment Variables
ENABLE_TORCH_COMPILE=1
# PyTorch Threading Optimization (Railway vCPU: 4 threads recommended)
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
TORCH_NUM_THREADS=4

# CPU Affinity (better cache locality)
KMP_AFFINITY=granularity=fine,compact,1,0
KMP_BLOCKTIME=0

# ONNX Runtime Optimization
ORT_NUM_THREADS=4
ORT_ENABLE_CPU_FP16_OPS=1

# Python Optimization
PYTHONDONTWRITEBYTECODE=1
PYTHONUNBUFFERED=1

# FastAPI/Uvicorn
UVICORN_WORKERS=1
UVICORN_PORT=11435
UVICORN_HOST=0.0.0.0

# Hugging Face Token (required for gated models like Gemma)
HF_TOKEN=hf_your_token_here

# Model Cache (Railway volume at /app/models)
# Note: TRANSFORMERS_CACHE is deprecated in transformers v5, use HF_HOME only
HF_HOME=/app/models

# ============================================================================
# Model Configuration
# ============================================================================

# Default models used when no model is specified in API requests
DEFAULT_EMBEDDING_MODEL=m2v-bge-m3-1024d
DEFAULT_RERANK_MODEL=qwen3-rerank

# HuggingFace Hub IDs for each model (override to use custom models)
HF_MODEL_M2V_BGE_M3=tss-deposium/m2v-bge-m3-1024d
HF_MODEL_BGE_M3_ONNX=gpahal/bge-m3-onnx-int8
HF_MODEL_GEMMA_768D=tss-deposium/gemma-deposium-768d
HF_MODEL_QWEN3_EMBED=Qwen/Qwen3-Embedding-0.6B

# ============================================================================
# VRAM Management
# ============================================================================

# Maximum VRAM to use in MB (default 5000 for 6GB GPU, keeps 1GB margin)
VRAM_LIMIT_MB=5000

# Time in seconds before unloading inactive models (default 180 = 3 minutes)
AUTO_UNLOAD_MODELS_TIME=180

# ============================================================================
# LLM Inference Backend Configuration
# ============================================================================

# Backend selection: huggingface (default), vllm_local, vllm_remote, remote_openai
# Leave empty for auto-detection
LLM_BACKEND=huggingface

# --- HuggingFace Backend Options ---
HF_DEVICE=cuda
HF_QUANTIZATION=bitsandbytes
HF_LOAD_4BIT=true
HF_FLASH_ATTENTION=true
HF_TRUST_REMOTE_CODE=true

# --- vLLM Local Backend Options ---
# (Requires: pip install vllm>=0.6.0, CUDA 12.1+, 8GB+ VRAM)
# VLLM_TENSOR_PARALLEL_SIZE=1        # Multi-GPU: 2, 4, 8
# VLLM_GPU_MEMORY_UTILIZATION=0.90   # Use 90% of GPU memory
# VLLM_QUANTIZATION=bitsandbytes     # or awq, gptq, marlin
# VLLM_MAX_MODEL_LEN=32768           # Context length override
# VLLM_DTYPE=auto                    # auto, half, float16, bfloat16
# VLLM_PREFIX_CACHING=true           # Cache repeated prompts
# VLLM_MAX_NUM_SEQS=256              # Max concurrent sequences
# VLLM_ENFORCE_EAGER=false           # Disable CUDA graphs for debugging

# --- vLLM Remote Backend Options ---
# VLLM_REMOTE_URL=http://gpu-server:8001/v1
# VLLM_REMOTE_API_KEY=your-api-key
# VLLM_REMOTE_TIMEOUT=120
# VLLM_REMOTE_MAX_RETRIES=3

# --- Remote OpenAI-compatible Backend Options ---
# REMOTE_OPENAI_URL=https://api.openai.com/v1
# REMOTE_OPENAI_API_KEY=your-api-key
# REMOTE_OPENAI_ORG=your-org-id
# REMOTE_OPENAI_MODEL=model-name-override
# REMOTE_OPENAI_TIMEOUT=120
# REMOTE_OPENAI_MAX_RETRIES=3

# ============================================================================
# Authentication
# ============================================================================

# API key for external requests (leave empty to disable auth in dev mode)
# EMBEDDINGS_API_KEY=your_secret_api_key_here
