# ============================================================================
# GPU Support (Docker build-time)
# ============================================================================
# Set via docker build --build-arg or docker-compose.gpu.yml
# ENABLE_GPU=false  → CPU-only PyTorch wheel (~200 MiB, default for Railway)
# ENABLE_GPU=true   → CUDA 12.6 PyTorch wheel (~2.5 GiB, requires nvidia runtime)
# See: deposium-local/docker-compose.gpu.yml for Docker Compose GPU override

# Performance Optimization Environment Variables
ENABLE_TORCH_COMPILE=1
# PyTorch Threading Optimization (Railway vCPU: 4 threads recommended)
# GPU mode: set to 4 (data prep); CPU mode: set to 2 (prevent 400% spikes)
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4
TORCH_NUM_THREADS=4

# CPU Affinity (better cache locality)
KMP_AFFINITY=granularity=fine,compact,1,0
KMP_BLOCKTIME=0

# ONNX Runtime Optimization
ORT_NUM_THREADS=4
ORT_ENABLE_CPU_FP16_OPS=1

# Python Optimization
PYTHONDONTWRITEBYTECODE=1
PYTHONUNBUFFERED=1

# FastAPI/Uvicorn
UVICORN_WORKERS=1
UVICORN_PORT=11435
UVICORN_HOST=0.0.0.0

# Hugging Face Token (required for gated models like Gemma)
HF_TOKEN=hf_your_token_here

# Model Cache (Railway volume at /app/models)
# Note: TRANSFORMERS_CACHE is deprecated in transformers v5, use HF_HOME only
HF_HOME=/app/models

# ============================================================================
# Model Configuration
# ============================================================================

# Default models used when no model is specified in API requests
DEFAULT_EMBEDDING_MODEL=m2v-bge-m3-1024d
DEFAULT_RERANK_MODEL=mxbai-rerank-v2

# HuggingFace Hub IDs for each model (override to use custom models)
HF_MODEL_M2V_BGE_M3=tss-deposium/m2v-bge-m3-1024d
HF_MODEL_BGE_M3_ONNX=gpahal/bge-m3-onnx-int8
HF_MODEL_BGE_M3_MATRYOSHKA=tss-deposium/bge-m3-matryoshka-1024d-onnx-int8
HF_MODEL_GEMMA_768D=tss-deposium/gemma-deposium-768d
HF_MODEL_QWEN3_EMBED=Qwen/Qwen3-Embedding-0.6B

# ============================================================================
# VRAM Management
# ============================================================================

# Maximum VRAM to use in MB (default 5000 for 6GB GPU, keeps 1GB margin)
VRAM_LIMIT_MB=5000

# Time in seconds before unloading inactive models (default 180 = 3 minutes)
AUTO_UNLOAD_MODELS_TIME=180

# ============================================================================
# LLM Inference Backend Configuration
# ============================================================================

# Backend selection: huggingface (default), vllm_local, vllm_remote, remote_openai, bitnet
# Leave empty for auto-detection
LLM_BACKEND=huggingface

# --- HuggingFace Backend Options ---
HF_DEVICE=cuda
HF_QUANTIZATION=bitsandbytes
HF_LOAD_4BIT=true
HF_FLASH_ATTENTION=true
# trust_remote_code: whitelisted prefixes (Qwen/, LiquidAI/, mixedbread-ai/) auto-trusted
# Set to true only if you need to load models from other providers
HF_TRUST_REMOTE_CODE=false

# --- vLLM Local Backend Options ---
# (Requires: pip install vllm>=0.6.0, CUDA 12.1+, 8GB+ VRAM)
# VLLM_TENSOR_PARALLEL_SIZE=1        # Multi-GPU: 2, 4, 8
# VLLM_GPU_MEMORY_UTILIZATION=0.90   # Use 90% of GPU memory
# VLLM_QUANTIZATION=bitsandbytes     # or awq, gptq, marlin
# VLLM_MAX_MODEL_LEN=32768           # Context length override
# VLLM_DTYPE=auto                    # auto, half, float16, bfloat16
# VLLM_PREFIX_CACHING=true           # Cache repeated prompts
# VLLM_MAX_NUM_SEQS=256              # Max concurrent sequences
# VLLM_ENFORCE_EAGER=false           # Disable CUDA graphs for debugging

# --- vLLM Remote Backend Options ---
# VLLM_REMOTE_URL=http://gpu-server:8001/v1
# VLLM_REMOTE_API_KEY=your-api-key
# VLLM_REMOTE_TIMEOUT=120
# VLLM_REMOTE_MAX_RETRIES=3

# --- Remote OpenAI-compatible Backend Options ---
# REMOTE_OPENAI_URL=https://api.openai.com/v1
# REMOTE_OPENAI_API_KEY=your-api-key
# REMOTE_OPENAI_ORG=your-org-id
# REMOTE_OPENAI_MODEL=model-name-override
# REMOTE_OPENAI_TIMEOUT=120
# REMOTE_OPENAI_MAX_RETRIES=3

# --- BitNet Backend Options (CPU-only 1-bit quantization) ---
# Microsoft BitNet: https://github.com/microsoft/BitNet
# Benefits: CPU-only, 2-6x speedup, 70-80% energy reduction, ~500MB for 2.4B model
# Ideal for Railway/edge deployments without GPU
# BITNET_PATH=/app/BitNet                           # Path to BitNet installation
# BITNET_MODEL_PATH=/app/models/bitnet/model.gguf   # Path to GGUF model file
# BITNET_THREADS=4                                   # CPU threads for inference
# BITNET_CONTEXT_LENGTH=2048                         # Maximum context length
# BITNET_TIMEOUT=120.0                               # Generation timeout (seconds)
# Available models: bitnet-700m, bitnet-2b, bitnet-llama3-8b

# ============================================================================
# Security & Middleware
# ============================================================================

# API key for external requests (leave empty to disable auth in dev mode)
# EMBEDDINGS_API_KEY=your_secret_api_key_here

# CORS - Comma-separated allowed origins (browser-only, not needed for server-to-server)
# Empty = no cross-origin allowed (most restrictive)
# Server-to-server calls (MCP, Edge runtime, .railway.internal) do NOT need CORS
# CORS_ALLOWED_ORIGINS=https://app.deposium.ai,https://app.deposium.vip
