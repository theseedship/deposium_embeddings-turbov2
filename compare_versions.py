#!/usr/bin/env python3
"""
LEAF Model Comparison: v1 (512 tokens) vs v2 (2048 tokens)

Compares MTEB performance, speed, and quality metrics between versions.
Generates comparison tables and visualizations.

Usage:
    python compare_versions.py --v1-model models/v1/model_quantized.pt \
                               --v2-model models/v2/model_quantized.pt \
                               --output comparison_report.md
"""

import torch
import numpy as np
from transformers import AutoTokenizer
from pathlib import Path
import time
import argparse
import json
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# MODEL WRAPPER
# ============================================================================
class LEAFModelWrapper:
    """Wrapper for LEAF model with MTEB compatibility"""

    def __init__(self, model_path: str, version: str = "v1"):
        self.version = version
        self.model_path = model_path
        logger.info(f"Loading {version} model from {model_path}")

        # Load model
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
        self.model = checkpoint['model']
        self.model.eval()

        # Load tokenizer
        tokenizer_path = Path(model_path).parent
        self.tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path))
        self.model.set_tokenizer(self.tokenizer)

        # Get model info
        self.max_seq_length = checkpoint.get('max_seq_length', 512 if version == "v1" else 2048)
        self.num_params = sum(p.numel() for p in self.model.parameters())

        logger.info(f"âœ… {version} model loaded: {self.num_params/1e6:.1f}M params, {self.max_seq_length} tokens")

    def encode(
        self,
        sentences: List[str],
        batch_size: int = 32,
        show_progress_bar: bool = False,
        normalize_embeddings: bool = True,
        **kwargs
    ) -> np.ndarray:
        """Encode sentences to embeddings (MTEB-compatible interface)"""
        all_embeddings = []

        for i in range(0, len(sentences), batch_size):
            batch = sentences[i:i + batch_size]

            with torch.no_grad():
                embeddings = self.model.encode(
                    batch,
                    device='cpu',
                    normalize=normalize_embeddings
                )

            all_embeddings.append(embeddings.cpu().numpy())

        return np.vstack(all_embeddings)


# ============================================================================
# BENCHMARKING
# ============================================================================
def benchmark_speed(model: LEAFModelWrapper, num_samples: int = 1000) -> Dict[str, float]:
    """Benchmark inference speed"""
    logger.info(f"Benchmarking {model.version} speed with {num_samples} samples...")

    # Generate test data
    texts = [f"This is a test sentence number {i} for speed benchmarking." for i in range(num_samples)]

    # Warmup
    _ = model.encode(texts[:10], batch_size=32)

    # Benchmark
    start_time = time.time()
    embeddings = model.encode(texts, batch_size=32, show_progress_bar=False)
    end_time = time.time()

    elapsed = end_time - start_time
    throughput = num_samples / elapsed
    latency = (elapsed / num_samples) * 1000  # ms

    results = {
        "throughput_texts_per_sec": throughput,
        "latency_ms_per_text": latency,
        "total_time_sec": elapsed,
        "embedding_dims": embeddings.shape[1],
    }

    logger.info(f"  Throughput: {throughput:.1f} texts/s")
    logger.info(f"  Latency: {latency:.2f} ms/text")

    return results


def benchmark_quality_mteb(model: LEAFModelWrapper, tasks: List[str] = None) -> Dict[str, Any]:
    """Run MTEB evaluation"""
    if tasks is None:
        tasks = ["STSBenchmark"]  # Quick task for comparison

    logger.info(f"Running MTEB evaluation on {model.version}...")

    try:
        from mteb import MTEB
    except ImportError:
        logger.error("MTEB not installed. Install with: pip install mteb")
        return {}

    evaluation = MTEB(tasks=tasks)
    results = evaluation.run(
        model,
        output_folder=f"comparison_results/{model.version}",
        eval_splits=["test"]
    )

    return results


# ============================================================================
# COMPARISON REPORT
# ============================================================================
def generate_comparison_report(
    v1_results: Dict[str, Any],
    v2_results: Dict[str, Any],
    output_path: str = "COMPARISON_REPORT.md"
):
    """Generate comprehensive comparison report"""
    logger.info(f"Generating comparison report: {output_path}")

    report = []

    # Header
    report.append("# LEAF Model Comparison: v1 vs v2\n")
    report.append("**Comparison Date**: 2025-10-12\n")
    report.append("**Evaluation**: MTEB + Speed Benchmarks\n")
    report.append("\n---\n")

    # Architecture Comparison
    report.append("## ðŸ“ Architecture Comparison\n")
    report.append("| Property | v1 (512 tokens) | v2 (2048 tokens) | Change |\n")
    report.append("|----------|-----------------|------------------|--------|\n")
    report.append(f"| **Layers** | 6 | 12 | +100% âœ… |\n")
    report.append(f"| **Parameters** | 75M | 120M | +60% âœ… |\n")
    report.append(f"| **Context Length** | 512 | 2048 | +300% âœ… |\n")
    report.append(f"| **Hidden Size Ratio** | 0.5x | 0.75x | +50% âœ… |\n")
    report.append(f"| **Training Data** | 50k samples | 200k samples | +300% âœ… |\n")
    report.append(f"| **Training Epochs** | 3 | 10 | +233% âœ… |\n")
    report.append(f"| **Alignment Loss Weight** | 1.0 | 2.5 | +150% âœ… |\n")
    report.append("\n")

    # Speed Comparison
    report.append("## âš¡ Speed Comparison\n")
    report.append("| Metric | v1 | v2 | Change |\n")
    report.append("|--------|----|----|--------|\n")

    v1_speed = v1_results.get("speed", {})
    v2_speed = v2_results.get("speed", {})

    v1_throughput = v1_speed.get("throughput_texts_per_sec", 0)
    v2_throughput = v2_speed.get("throughput_texts_per_sec", 0)
    throughput_change = ((v2_throughput - v1_throughput) / v1_throughput * 100) if v1_throughput > 0 else 0

    v1_latency = v1_speed.get("latency_ms_per_text", 0)
    v2_latency = v2_speed.get("latency_ms_per_text", 0)
    latency_change = ((v2_latency - v1_latency) / v1_latency * 100) if v1_latency > 0 else 0

    report.append(f"| **Throughput** | {v1_throughput:.1f} texts/s | {v2_throughput:.1f} texts/s | {throughput_change:+.1f}% |\n")
    report.append(f"| **Latency** | {v1_latency:.2f} ms | {v2_latency:.2f} ms | {latency_change:+.1f}% |\n")
    report.append(f"| **Embedding Dims** | {v1_speed.get('embedding_dims', 768)} | {v2_speed.get('embedding_dims', 768)} | Same |\n")
    report.append("\n")

    # Quality Comparison (MTEB)
    report.append("## ðŸ“Š Quality Comparison (MTEB)\n")

    # Check if we have MTEB results
    v1_mteb = v1_results.get("mteb", {})
    v2_mteb = v2_results.get("mteb", {})

    if v1_mteb or v2_mteb:
        report.append("| Task | Metric | v1 | v2 | Improvement |\n")
        report.append("|------|--------|----|----|-------------|\n")

        # STSBenchmark
        v1_sts = v1_mteb.get("STSBenchmark", {}).get("test", {}).get("spearman", 0.223)
        v2_sts = v2_mteb.get("STSBenchmark", {}).get("test", {}).get("spearman", 0)
        sts_improvement = ((v2_sts - v1_sts) / v1_sts * 100) if v1_sts > 0 else 0

        report.append(f"| **STSBenchmark** | Spearman | {v1_sts:.3f} | {v2_sts:.3f} | {sts_improvement:+.1f}% |\n")

    else:
        # Use known v1 results
        report.append("### Known Results (v1)\n\n")
        report.append("| Task | Metric | v1 (FAILED) | v2 (Target) | Target Improvement |\n")
        report.append("|------|--------|-------------|-------------| -------------------|\n")
        report.append(f"| **STSBenchmark** | Spearman | 0.223 | 0.70+ | **+214%** ðŸŽ¯ |\n")
        report.append(f"| **STS22 English** | Spearman | 0.373 | 0.65+ | **+74%** ðŸŽ¯ |\n")
        report.append(f"| **STS22 Average** | Spearman | ~0.21 | 0.50+ | **+138%** ðŸŽ¯ |\n")
        report.append(f"| **Cross-lingual** | Spearman | -0.14 | 0.30+ | **Complete Fix** ðŸŽ¯ |\n")
        report.append(f"| **MTEB Score (est.)** | Overall | ~25 | 55+ | **+120%** ðŸŽ¯ |\n")

    report.append("\n")

    # Detailed v1 Results (Known)
    report.append("## âŒ Detailed v1 Results (FAILED)\n\n")
    report.append("### STSBenchmark\n")
    report.append("- **Spearman**: 0.223 (Target: 0.81)\n")
    report.append("- **Quality Loss**: -72% vs base model\n")
    report.append("- **Status**: âŒ CRITICAL FAILURE\n\n")

    report.append("### STS22 by Language\n")
    report.append("| Language | Spearman | Status |\n")
    report.append("|----------|----------|--------|\n")
    report.append("| ðŸ‡¨ðŸ‡³ Chinese | 0.499 | ðŸŸ¡ Best (still poor) |\n")
    report.append("| ðŸ‡¸ðŸ‡¦ Arabic | 0.469 | ðŸŸ¡ Moderate |\n")
    report.append("| ðŸ‡®ðŸ‡¹ Italian | 0.435 | ðŸŸ¡ Moderate |\n")
    report.append("| ðŸ‡ªðŸ‡¸ Spanish | 0.403 | ðŸŸ  Poor |\n")
    report.append("| ðŸ‡¬ðŸ‡§ English | 0.373 | ðŸŸ  Poor |\n")
    report.append("| ðŸ‡«ðŸ‡· French | 0.300 | ðŸ”´ Very poor |\n")
    report.append("| ðŸ‡·ðŸ‡º Russian | 0.268 | ðŸ”´ Very poor |\n")
    report.append("| ðŸ‡¹ðŸ‡· Turkish | 0.247 | ðŸ”´ Very poor |\n")
    report.append("| ðŸ‡©ðŸ‡ª German | 0.163 | âŒ Critical |\n")
    report.append("| ðŸ‡µðŸ‡± Polish | 0.132 | âŒ Critical |\n\n")

    report.append("### Cross-lingual (Translation Tasks)\n")
    report.append("| Pair | Spearman | Status |\n")
    report.append("|------|----------|--------|\n")
    report.append("| ðŸ‡ªðŸ‡¸-ðŸ‡®ðŸ‡¹ | 0.119 | âŒ Failed |\n")
    report.append("| ðŸ‡©ðŸ‡ª-ðŸ‡µðŸ‡± | 0.113 | âŒ Failed |\n")
    report.append("| ðŸ‡©ðŸ‡ª-ðŸ‡«ðŸ‡· | 0.070 | âŒ Failed |\n")
    report.append("| ðŸ‡ªðŸ‡¸-ðŸ‡¬ðŸ‡§ | 0.002 | âŒ Random |\n")
    report.append("| ðŸ‡¨ðŸ‡³-ðŸ‡¬ðŸ‡§ | -0.012 | âŒ Inverse |\n")
    report.append("| ðŸ‡µðŸ‡±-ðŸ‡¬ðŸ‡§ | -0.143 | âŒ **WORST** |\n\n")

    # Summary
    report.append("## ðŸ“ Summary\n\n")
    report.append("### v1 (512 tokens) - FAILED\n")
    report.append("- âŒ **Architecture too aggressive**: 6 layers insufficient\n")
    report.append("- âŒ **Data insufficient**: 50k samples, mostly English\n")
    report.append("- âŒ **High alignment loss**: 2.18 (warning sign)\n")
    report.append("- âŒ **Quality catastrophic**: -72% vs base model\n")
    report.append("- âŒ **Multilingual destroyed**: Cross-lingual scores negative\n")
    report.append("- âœ… **Speed excellent**: 695 texts/s\n\n")

    report.append("### v2 (2048 tokens) - Expected Improvements\n")
    report.append("- âœ… **Architecture doubled**: 12 layers (2x)\n")
    report.append("- âœ… **Data 4x larger**: 200k multilingual samples\n")
    report.append("- âœ… **Alignment prioritized**: Weight 2.5 (vs 1.0)\n")
    report.append("- âœ… **Curriculum learning**: 512â†’1024â†’2048 progressive\n")
    report.append("- âœ… **Quality monitoring**: MTEB validation every 1000 steps\n")
    report.append("- âœ… **Target**: MTEB 55+ (vs ~25 in v1)\n")
    report.append("- âš ï¸ **Speed trade-off**: Likely ~400-500 texts/s (still fast)\n\n")

    # Recommendations
    report.append("## ðŸŽ¯ Recommendations\n\n")
    report.append("1. **Proceed with v2 training** using the improved configuration\n")
    report.append("2. **Monitor alignment loss** - stop if > 1.5 after epoch 3\n")
    report.append("3. **Validate frequently** - MTEB STSBenchmark every 1000 steps\n")
    report.append("4. **Target quality** - Spearman 0.70+ on STSBenchmark\n")
    report.append("5. **Expected training time** - 12-15 hours on RTX 4050\n\n")

    # Write report
    report_text = "".join(report)
    Path(output_path).write_text(report_text)
    logger.info(f"âœ… Comparison report saved to {output_path}")

    return report_text


# ============================================================================
# MAIN
# ============================================================================
def main():
    parser = argparse.ArgumentParser(description="Compare LEAF v1 vs v2 models")
    parser.add_argument("--v1-model", type=str, help="Path to v1 model")
    parser.add_argument("--v2-model", type=str, help="Path to v2 model (optional)")
    parser.add_argument("--output", type=str, default="COMPARISON_REPORT.md", help="Output report path")
    parser.add_argument("--benchmark-speed", action="store_true", help="Run speed benchmarks")
    parser.add_argument("--benchmark-mteb", action="store_true", help="Run MTEB benchmarks (slow)")
    args = parser.parse_args()

    results = {
        "v1": {},
        "v2": {}
    }

    # Load v1 model (if provided)
    if args.v1_model and Path(args.v1_model).exists():
        v1_model = LEAFModelWrapper(args.v1_model, version="v1")

        if args.benchmark_speed:
            results["v1"]["speed"] = benchmark_speed(v1_model)

        if args.benchmark_mteb:
            results["v1"]["mteb"] = benchmark_quality_mteb(v1_model)

    # Load v2 model (if provided)
    if args.v2_model and Path(args.v2_model).exists():
        v2_model = LEAFModelWrapper(args.v2_model, version="v2")

        if args.benchmark_speed:
            results["v2"]["speed"] = benchmark_speed(v2_model)

        if args.benchmark_mteb:
            results["v2"]["mteb"] = benchmark_quality_mteb(v2_model)

    # Generate report
    report = generate_comparison_report(results["v1"], results["v2"], args.output)

    logger.info("âœ… Comparison complete!")
    logger.info(f"ðŸ“„ Report: {args.output}")


if __name__ == "__main__":
    main()
