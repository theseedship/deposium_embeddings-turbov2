# üìä Benchmarks - Comparaison des Mod√®les

Ce dossier contient tous les benchmarks et comparaisons des mod√®les d'embeddings test√©s pour Deposium.

## üéØ Tableau Comparatif - Mod√®les Principaux

| Mod√®le | Dimensions | Taille | Qualit√© Globale | Instruction-Aware | Recommandation | Statut |
|--------|------------|--------|-----------------|-------------------|----------------|---------|
| **qwen25-1024d** ‚≠ê | 1024 | 65MB | **68.2%** | ‚úÖ **94.9%** | ‚úÖ Deploy | **PRODUCTION** |
| **gemma-768d** | 768 | 400MB | 65.9% | ‚ùå | ‚úÖ Deploy | Backup |
| **qwen3-1024d** | 1024 | 600MB | 37.5% | ‚ùå | ‚ùå Do not deploy | Rejected |
| **qwen3-256d** | 256 | 100MB | 66.5% | ‚ùå | ‚ö†Ô∏è OK (limit√©) | Archive |
| **granite-4.0-micro** | - | - | ~86% (multilingual) | ‚ùå | ‚ö†Ô∏è Test only | Experimental |

### üèÜ Mod√®le Recommand√©: **qwen25-1024d**

**Pourquoi ?**
- ‚úÖ **Instruction-aware unique** (94.9%) - SEUL mod√®le avec cette capacit√©
- ‚úÖ **Ultra compact** (65MB vs 400-600MB)
- ‚úÖ **Qualit√© competitive** (68.2%)
- ‚úÖ **Code understanding** (84.5%)
- ‚úÖ **Conversational** (80.0%)
- ‚úÖ **Multilingual** (39.4%)

## üìÅ Structure des R√©sultats

```
benchmarks/
‚îú‚îÄ‚îÄ README.md                      # Ce fichier - Vue d'ensemble
‚îú‚îÄ‚îÄ model_comparison_results.json  # Comparaison globale
‚îú‚îÄ‚îÄ comparison_results.txt         # R√©sultats texte
‚îÇ
‚îú‚îÄ‚îÄ models/                        # R√©sultats d√©taill√©s par mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ gemma-768d/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ results.json          # Scores d√©taill√©s
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_script.py        # Script d'√©valuation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logs/                 # Logs d'ex√©cution
‚îÇ   ‚îú‚îÄ‚îÄ qwen25-1024d/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ results.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eval_script.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ distill_qwen25_1024d.py
‚îÇ   ‚îú‚îÄ‚îÄ qwen25-7b/
‚îÇ   ‚îú‚îÄ‚îÄ qwen3/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qwen3_1024d_eval_results.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qwen3_256d_eval_results.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qwen3_quick_eval_results.json
‚îÇ   ‚îî‚îÄ‚îÄ granite/
‚îÇ       ‚îî‚îÄ‚îÄ results.txt
‚îÇ
‚îú‚îÄ‚îÄ comparisons/                   # Scripts de comparaison
‚îÇ   ‚îú‚îÄ‚îÄ compare_all_models.py     # Compare tous les mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ compare_baseline_vs_qwen25.py
‚îÇ   ‚îú‚îÄ‚îÄ compare_models_mteb.py
‚îÇ   ‚îú‚îÄ‚îÄ compare_qwen25_vs_all.py
‚îÇ   ‚îî‚îÄ‚îÄ compare_versions.py
‚îÇ
‚îú‚îÄ‚îÄ mteb/                         # R√©sultats MTEB (benchmarks standard)
‚îÇ   ‚îú‚îÄ‚îÄ results/                  # R√©sultats complets
‚îÇ   ‚îú‚îÄ‚îÄ results_baseline/         # Baseline de r√©f√©rence
‚îÇ   ‚îî‚îÄ‚îÄ results_quick/            # Tests rapides
‚îÇ
‚îî‚îÄ‚îÄ tools/                        # Outils de benchmarking
    ‚îú‚îÄ‚îÄ benchmark.py
    ‚îú‚îÄ‚îÄ benchmark.sh
    ‚îú‚îÄ‚îÄ benchmark-simple.sh
    ‚îú‚îÄ‚îÄ benchmark_onnx.py
    ‚îú‚îÄ‚îÄ monitor_baseline.sh
    ‚îú‚îÄ‚îÄ monitor_granite.sh
    ‚îú‚îÄ‚îÄ monitor_mteb_live.sh
    ‚îî‚îÄ‚îÄ extract_mteb_scores.sh
```

## üìà D√©tails par Mod√®le

### üî• Qwen25-1024D (Production)

**Scores d√©taill√©s:**
- Semantic Similarity: 54.2%
- Topic Clustering: 43.4%
- Multilingual Alignment: 39.4%
- **Instruction Awareness: 94.9%** ‚≠ê (unique)
- **Conversational Understanding: 80.0%**
- **Code Understanding: 84.5%**

**Fichiers:**
- `models/qwen25-1024d/results.json`
- `models/qwen25-1024d/eval_script.py`
- `models/qwen25-1024d/distill_qwen25_1024d.py`

### ‚ö° Gemma-768D (Backup)

**Scores d√©taill√©s:**
- Overall Quality: 65.9%
- Semantic Similarity: 73.0%
- Topic Clustering: 55.6%
- **Multilingual Alignment: 69.0%** (meilleur)
- Silhouette Score: 0.11
- Cluster Purity: 100%

**Fichiers:**
- `models/gemma-768d/results.json`
- `models/gemma-768d/eval_script.py`

### ‚ùå Qwen3-1024D (Rejected)

**Pourquoi rejet√©:**
- Overall Quality: **37.5%** (trop faible)
- Semantic Similarity: 57.1%
- Topic Clustering: 35.0%
- Multilingual Alignment: **20.3%** (tr√®s faible)
- **-43.7% vs qwen3-256d** (r√©gression massive)

**Fichiers:**
- `models/qwen3/qwen3_1024d_eval_results.json`

### üß™ Granite 4.0 Micro (Experimental)

**Test multilingual:**
- English: 93.5%
- French: 94.0%
- German: 89.9%
- Spanish: 73.2%

**Fichiers:**
- `models/granite/results.txt`

## üõ†Ô∏è Comment Ajouter un Nouveau Mod√®le

1. **Cr√©er le dossier:**
   ```bash
   mkdir -p benchmarks/models/nom-du-modele/logs
   ```

2. **Copier le script d'√©valuation:**
   ```bash
   cp benchmarks/models/qwen25-1024d/eval_script.py benchmarks/models/nom-du-modele/
   # Adapter le script au nouveau mod√®le
   ```

3. **Lancer l'√©valuation:**
   ```bash
   python benchmarks/models/nom-du-modele/eval_script.py > benchmarks/models/nom-du-modele/results.json
   ```

4. **Comparer avec les autres:**
   ```bash
   python benchmarks/comparisons/compare_all_models.py
   ```

5. **Mettre √† jour ce README** avec les nouveaux r√©sultats

## üìä Scripts de Comparaison

### `compare_all_models.py`
Compare tous les mod√®les disponibles et g√©n√®re un rapport complet.

### `compare_baseline_vs_qwen25.py`
Compare sp√©cifiquement qwen25 avec la baseline de r√©f√©rence.

### `compare_models_mteb.py`
Compare les mod√®les en utilisant le benchmark MTEB standard.

### `compare_qwen25_vs_all.py`
Compare qwen25 avec tous les autres mod√®les (d√©taill√©).

### `compare_versions.py`
Compare diff√©rentes versions d'un m√™me mod√®le.

## üéØ MTEB Benchmarks

Les r√©sultats MTEB (Massive Text Embedding Benchmark) sont dans `mteb/`:

- **results/** - √âvaluations compl√®tes MTEB
- **results_baseline/** - Scores de r√©f√©rence (sentence-transformers)
- **results_quick/** - Tests rapides (sous-ensemble de t√¢ches)

Documentation MTEB: `docs/guides/mteb/`

## üîß Outils de Monitoring

### Scripts de monitoring en temps r√©el:
- `tools/monitor_baseline.sh` - Monitor baseline evaluation
- `tools/monitor_granite.sh` - Monitor granite evaluation
- `tools/monitor_mteb_live.sh` - Monitor MTEB runs

### Extraction de scores:
- `tools/extract_mteb_scores.sh` - Extraire les scores MTEB des r√©sultats

## üìö Documentation Associ√©e

- **Guides MTEB:** `docs/guides/mteb/`
  - MTEB_GUIDE.md
  - MTEB_QUICKSTART.md
  - MTEB_FINAL_ANALYSIS.md
  - MTEB_RESULTS_SUMMARY.md

- **Analyses benchmarks:** `docs/analysis/benchmarks/`
  - BENCHMARK_RESULTS.md
  - COMPARISON_REPORT.md

- **Analyses mod√®les:** `docs/analysis/models/`
  - MODEL_ANALYSIS.md
  - MODEL2VEC_STRATEGY.md

## üöÄ Prochaines √âtapes

1. **Nouveau mod√®le √† tester ?** Suivre la section "Comment Ajouter un Nouveau Mod√®le"
2. **Comparer les r√©sultats ?** Utiliser `benchmarks/comparisons/compare_all_models.py`
3. **MTEB complet ?** Voir `docs/guides/mteb/MTEB_QUICKSTART.md`

## üí° Notes

- **Instruction-awareness** est LA capacit√© unique de qwen25-1024d
- **Taille du mod√®le** est critique pour Railway (limites m√©moire)
- **Multilingual** n'est pas prioritaire (focus fran√ßais/anglais)
- **MTEB scores** donnent la vue d'ensemble sur tasks standards

---

üìä **Derni√®re mise √† jour:** 2025-10-18
‚ú® **Mod√®le en production:** qwen25-1024d (65MB, instruction-aware)