# ğŸ“Š Benchmarks - Comparaison des ModÃ¨les

Ce dossier contient tous les benchmarks et comparaisons des modÃ¨les d'embeddings testÃ©s pour Deposium.

## ğŸ¯ Tableau Comparatif - ModÃ¨les Principaux

| ModÃ¨le | Dimensions | Taille | QualitÃ© Globale | Instruction-Aware | Code Understanding | Recommandation | Statut |
|--------|------------|--------|-----------------|-------------------|-------------------|----------------|---------|
| **qwen25-1024d** â­ | 1024 | 65MB | **68.2%** | âœ… **94.9%** | âœ… **84.5%** | âœ… Deploy | **PRODUCTION** |
| **mxbai-edge-colbert-32m** | Multi-vector | 964MB | **94.4%** | âœ… **95.6%** | âœ… **94.0%** | âŒ Rejected (RAM) | Tested |
| **gemma-768d** | 768 | 400MB | 65.9% | âŒ | âŒ | âœ… Deploy | Backup |
| **qwen3-1024d** | 1024 | 600MB | 37.5% | âŒ | âŒ | âŒ Do not deploy | Rejected |
| **qwen3-256d** | 256 | 100MB | 66.5% | âŒ | âŒ | âš ï¸ OK (limitÃ©) | Archive |
| **granite-4.0-micro** | - | - | ~86% (multilingual) | âŒ | âŒ | âš ï¸ Test only | Experimental |

### ğŸ† ModÃ¨le RecommandÃ©: **qwen25-1024d**

**Pourquoi ?**
- âœ… **Instruction-aware unique** (94.9%) - SEUL modÃ¨le avec cette capacitÃ©
- âœ… **Ultra compact** (65MB vs 400-600MB)
- âœ… **QualitÃ© competitive** (68.2%)
- âœ… **Code understanding** (84.5%)
- âœ… **Conversational** (80.0%)
- âœ… **Multilingual** (39.4%)

## ğŸ“ Structure des RÃ©sultats

```
benchmarks/
â”œâ”€â”€ README.md                      # Ce fichier - Vue d'ensemble
â”œâ”€â”€ model_comparison_results.json  # Comparaison globale
â”œâ”€â”€ comparison_results.txt         # RÃ©sultats texte
â”‚
â”œâ”€â”€ models/                        # RÃ©sultats dÃ©taillÃ©s par modÃ¨le
â”‚   â”œâ”€â”€ gemma-768d/
â”‚   â”‚   â”œâ”€â”€ results.json          # Scores dÃ©taillÃ©s
â”‚   â”‚   â”œâ”€â”€ eval_script.py        # Script d'Ã©valuation
â”‚   â”‚   â””â”€â”€ logs/                 # Logs d'exÃ©cution
â”‚   â”œâ”€â”€ qwen25-1024d/
â”‚   â”‚   â”œâ”€â”€ results.json
â”‚   â”‚   â”œâ”€â”€ eval_script.py
â”‚   â”‚   â””â”€â”€ distill_qwen25_1024d.py
â”‚   â”œâ”€â”€ qwen25-7b/
â”‚   â”œâ”€â”€ qwen3/
â”‚   â”‚   â”œâ”€â”€ qwen3_1024d_eval_results.json
â”‚   â”‚   â”œâ”€â”€ qwen3_256d_eval_results.json
â”‚   â”‚   â””â”€â”€ qwen3_quick_eval_results.json
â”‚   â””â”€â”€ granite/
â”‚       â””â”€â”€ results.txt
â”‚
â”œâ”€â”€ comparisons/                   # Scripts de comparaison
â”‚   â”œâ”€â”€ compare_all_models.py     # Compare tous les modÃ¨les
â”‚   â”œâ”€â”€ compare_baseline_vs_qwen25.py
â”‚   â”œâ”€â”€ compare_models_mteb.py
â”‚   â”œâ”€â”€ compare_qwen25_vs_all.py
â”‚   â””â”€â”€ compare_versions.py
â”‚
â”œâ”€â”€ mteb/                         # RÃ©sultats MTEB (benchmarks standard)
â”‚   â”œâ”€â”€ results/                  # RÃ©sultats complets
â”‚   â”œâ”€â”€ results_baseline/         # Baseline de rÃ©fÃ©rence
â”‚   â””â”€â”€ results_quick/            # Tests rapides
â”‚
â””â”€â”€ tools/                        # Outils de benchmarking
    â”œâ”€â”€ benchmark.py
    â”œâ”€â”€ benchmark.sh
    â”œâ”€â”€ benchmark-simple.sh
    â”œâ”€â”€ benchmark_onnx.py
    â”œâ”€â”€ monitor_baseline.sh
    â”œâ”€â”€ monitor_granite.sh
    â”œâ”€â”€ monitor_mteb_live.sh
    â””â”€â”€ extract_mteb_scores.sh
```

## ğŸ“ˆ DÃ©tails par ModÃ¨le

### ğŸ”¥ Qwen25-1024D (Production)

**Scores dÃ©taillÃ©s:**
- Semantic Similarity: 54.2%
- Topic Clustering: 43.4%
- Multilingual Alignment: 39.4%
- **Instruction Awareness: 94.9%** â­ (unique)
- **Conversational Understanding: 80.0%**
- **Code Understanding: 84.5%**

**Fichiers:**
- `models/qwen25-1024d/results.json`
- `models/qwen25-1024d/eval_script.py`
- `models/qwen25-1024d/distill_qwen25_1024d.py`

### ğŸ” mxbai-edge-colbert-32m (Tested - Rejected for RAM)

**Architecture:** Multi-vector (ColBERT) - Late interaction, token-level matching

**Scores dÃ©taillÃ©s:**
- **Overall Quality: 94.4%** ğŸ¯ (meilleur testÃ©!)
- **Semantic Similarity: 93.6%**
- **Instruction Awareness: 95.6%** (+0.7% vs qwen25)
- **Code Understanding: 94.0%** (+9.5% vs qwen25) ğŸš€
- **Multilingue: FR 91.97%, ES 92.47%, DE 90.26%** (< 4% dÃ©gradation vs EN)
- **Encoding Speed: 5.94 ms/text** (trÃ¨s rapide)
- Model Size: 964 MB
- RAM Total: 1.38 GB

**Pourquoi rejetÃ©:**
- âŒ **+964MB RAM overhead** (15x plus gros que qwen25: 65MB)
- âŒ Multi-vector incompatible avec stack actuelle
- âŒ Distillation Model2Vec impossible (architecture incompatible)
- âŒ Rapport qualitÃ©/RAM: 0.098% /MB vs qwen25: 1.05% /MB (10.7x moins efficient)

**Conclusion:**
- âœ… Excellente qualitÃ© technique (+26.2% vs qwen25)
- âŒ Overhead infrastructure trop important pour notre use case edge
- ğŸ“š **ArchivÃ© comme rÃ©fÃ©rence "gold standard" (94.4%)**
- ğŸ¯ Confirme que qwen25-1024d est un excellent compromis

**Fichiers:**
- `models/mxbai-edge-colbert-32m/DECISION.md` â† **Document de dÃ©cision complet**
- `models/mxbai-edge-colbert-32m/results.txt`
- `models/mxbai-edge-colbert-32m/test_colbert.py`
- `models/mxbai-edge-colbert-32m/test_multilingual.py`
- `models/mxbai-edge-colbert-32m/README.md`
- `COLBERT_TESTING.md` (guide complet)

### âš¡ Gemma-768D (Backup)

**Scores dÃ©taillÃ©s:**
- Overall Quality: 65.9%
- Semantic Similarity: 73.0%
- Topic Clustering: 55.6%
- **Multilingual Alignment: 69.0%** (meilleur)
- Silhouette Score: 0.11
- Cluster Purity: 100%

**Fichiers:**
- `models/gemma-768d/results.json`
- `models/gemma-768d/eval_script.py`

### âŒ Qwen3-1024D (Rejected)

**Pourquoi rejetÃ©:**
- Overall Quality: **37.5%** (trop faible)
- Semantic Similarity: 57.1%
- Topic Clustering: 35.0%
- Multilingual Alignment: **20.3%** (trÃ¨s faible)
- **-43.7% vs qwen3-256d** (rÃ©gression massive)

**Fichiers:**
- `models/qwen3/qwen3_1024d_eval_results.json`

### ğŸ§ª Granite 4.0 Micro (Experimental)

**Test multilingual:**
- English: 93.5%
- French: 94.0%
- German: 89.9%
- Spanish: 73.2%

**Fichiers:**
- `models/granite/results.txt`

## ğŸ› ï¸ Comment Ajouter un Nouveau ModÃ¨le

1. **CrÃ©er le dossier:**
   ```bash
   mkdir -p benchmarks/models/nom-du-modele/logs
   ```

2. **Copier le script d'Ã©valuation:**
   ```bash
   cp benchmarks/models/qwen25-1024d/eval_script.py benchmarks/models/nom-du-modele/
   # Adapter le script au nouveau modÃ¨le
   ```

3. **Lancer l'Ã©valuation:**
   ```bash
   python benchmarks/models/nom-du-modele/eval_script.py > benchmarks/models/nom-du-modele/results.json
   ```

4. **Comparer avec les autres:**
   ```bash
   python benchmarks/comparisons/compare_all_models.py
   ```

5. **Mettre Ã  jour ce README** avec les nouveaux rÃ©sultats

## ğŸ“Š Scripts de Comparaison

### `compare_all_models.py`
Compare tous les modÃ¨les disponibles et gÃ©nÃ¨re un rapport complet.

### `compare_baseline_vs_qwen25.py`
Compare spÃ©cifiquement qwen25 avec la baseline de rÃ©fÃ©rence.

### `compare_models_mteb.py`
Compare les modÃ¨les en utilisant le benchmark MTEB standard.

### `compare_qwen25_vs_all.py`
Compare qwen25 avec tous les autres modÃ¨les (dÃ©taillÃ©).

### `compare_versions.py`
Compare diffÃ©rentes versions d'un mÃªme modÃ¨le.

## ğŸ¯ MTEB Benchmarks

Les rÃ©sultats MTEB (Massive Text Embedding Benchmark) sont dans `mteb/`:

- **results/** - Ã‰valuations complÃ¨tes MTEB
- **results_baseline/** - Scores de rÃ©fÃ©rence (sentence-transformers)
- **results_quick/** - Tests rapides (sous-ensemble de tÃ¢ches)

Documentation MTEB: `docs/guides/mteb/`

## ğŸ”§ Outils de Monitoring

### Scripts de monitoring en temps rÃ©el:
- `tools/monitor_baseline.sh` - Monitor baseline evaluation
- `tools/monitor_granite.sh` - Monitor granite evaluation
- `tools/monitor_mteb_live.sh` - Monitor MTEB runs

### Extraction de scores:
- `tools/extract_mteb_scores.sh` - Extraire les scores MTEB des rÃ©sultats

## ğŸ“š Documentation AssociÃ©e

- **Guides MTEB:** `docs/guides/mteb/`
  - MTEB_GUIDE.md
  - MTEB_QUICKSTART.md
  - MTEB_FINAL_ANALYSIS.md
  - MTEB_RESULTS_SUMMARY.md

- **Analyses benchmarks:** `docs/analysis/benchmarks/`
  - BENCHMARK_RESULTS.md
  - COMPARISON_REPORT.md

- **Analyses modÃ¨les:** `docs/analysis/models/`
  - MODEL_ANALYSIS.md
  - MODEL2VEC_STRATEGY.md

## ğŸš€ Prochaines Ã‰tapes

1. **Nouveau modÃ¨le Ã  tester ?** Suivre la section "Comment Ajouter un Nouveau ModÃ¨le"
2. **Comparer les rÃ©sultats ?** Utiliser `benchmarks/comparisons/compare_all_models.py`
3. **MTEB complet ?** Voir `docs/guides/mteb/MTEB_QUICKSTART.md`

## ğŸ’¡ Notes

- **Instruction-awareness** est LA capacitÃ© unique de qwen25-1024d
- **Taille du modÃ¨le** est critique pour Railway (limites mÃ©moire)
- **Multilingual** n'est pas prioritaire (focus franÃ§ais/anglais)
- **MTEB scores** donnent la vue d'ensemble sur tasks standards

---

ğŸ“Š **DerniÃ¨re mise Ã  jour:** 2025-10-18
âœ¨ **ModÃ¨le en production:** qwen25-1024d (65MB, instruction-aware)