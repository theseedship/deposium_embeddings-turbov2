# üîÑ Int√©gration du Mod√®le int8 (256D)

## üìä Vue d'Ensemble

**Ajout du second mod√®le C10X/int8 au service d'embeddings existant.**

### Mod√®les Disponibles

| Mod√®le | Dimensions | Base | Utilisation |
|--------|------------|------|-------------|
| **turbov2** | 1024D | Qwen3-Embedding-0.6B | Embeddings g√©n√©raux, recherche s√©mantique |
| **int8** | 256D | Qwen3-Reranker-0.6B | Embeddings l√©gers, optimisation reranking |

---

## üõ†Ô∏è Modifications Apport√©es

### 1. `src/main.py`

**Changements principaux:**

```python
# Avant: Un seul mod√®le
model = StaticModel.from_pretrained("C10X/Qwen3-Embedding-TurboX.v2")

# Apr√®s: Deux mod√®les dans un dictionnaire
models = {
    "turbov2": StaticModel.from_pretrained("C10X/Qwen3-Embedding-TurboX.v2"),
    "int8": StaticModel.from_pretrained("C10X/int8")
}
```

**Endpoint `/api/embed` modifi√©:**
- Validation du mod√®le s√©lectionn√©
- S√©lection dynamique du mod√®le
- Logging des dimensions g√©n√©r√©es

**Endpoints mis √† jour:**
- `GET /` - Affiche les 2 mod√®les disponibles
- `GET /health` - V√©rifie le chargement des 2 mod√®les
- `GET /api/tags` - Liste les 2 mod√®les (Ollama-compatible)

### 2. `README.md`

**Documentation compl√®te:**
- Section "Available Models" avec d√©tails des 2 mod√®les
- Exemples de curl pour chaque mod√®le
- Configuration N8N pour les 2 mod√®les
- Dimensions clarifi√©es (1024D vs 256D)

---

## üß™ Tests de Validation

### Test Local

```bash
# 1. Rebuild du container
docker build -t deposium-embeddings-turbov2 .

# 2. D√©marrer le service
docker run -p 11435:11435 deposium-embeddings-turbov2

# 3. V√©rifier le chargement des mod√®les
curl http://localhost:11435/health

# Expected:
{
  "status": "healthy",
  "models_loaded": ["turbov2", "int8"]
}

# 4. Lister les mod√®les
curl http://localhost:11435/api/tags

# Expected: 2 mod√®les list√©s

# 5. Test TurboX.v2 (1024D)
curl -X POST http://localhost:11435/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model":"turbov2","input":"test"}'

# Expected: embeddings avec 1024 dimensions

# 6. Test int8 (256D)
curl -X POST http://localhost:11435/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model":"int8","input":"test"}'

# Expected: embeddings avec 256 dimensions
```

### Test Railway

```bash
# 1. Push to GitHub
git add .
git commit -m "feat: add int8 (256D) model support"
git push origin main

# 2. Railway auto-deploy

# 3. Test HTTPS
curl -X POST https://deposiumembeddings-turbov2-staging.up.railway.app/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model":"int8","input":"test"}'

# 4. Test Private Network (sans port)
curl -X POST http://deposium-embeddings-turbov2.railway.internal/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model":"int8","input":"test"}'
```

---

## üìù Configuration N8N

### Cr√©er 2 Credentials Ollama

**Credential 1: TurboX.v2 (1024D)**
```
Name: Deposium Embeddings - TurboX.v2
Base URL: http://deposium-embeddings-turbov2:11435
Model: turbov2
```

**Credential 2: int8 (256D)**
```
Name: Deposium Embeddings - int8
Base URL: http://deposium-embeddings-turbov2:11435
Model: int8
```

### Cas d'Usage

**Utiliser TurboX.v2 (1024D) pour:**
- Recherche s√©mantique principale
- Embeddings de haute qualit√©
- T√¢ches n√©cessitant plus de dimensions

**Utiliser int8 (256D) pour:**
- Embeddings l√©gers (√©conomie de stockage)
- Pr√©-filtrage rapide avant reranking
- Cas o√π 256D suffisent (classification simple)

---

## üîç Architecture Technique

### Pipeline int8

```
Input Text
    ‚Üì
Qwen3-Reranker-0.6B Tokenizer
    ‚Üì
Model2Vec Static Model
    ‚Üì PCA: 2048D ‚Üí 256D
    ‚Üì SIF Weighting
    ‚Üì L2 Normalization
256D Embedding Vector
```

### Diff√©rences avec TurboX.v2

| Aspect | TurboX.v2 | int8 |
|--------|-----------|------|
| Tokenizer | Qwen3-Embedding-0.6B | Qwen3-Reranker-0.6B |
| PCA Source | 2048D | 2048D |
| Output Dims | 1024D | 256D |
| Use Case | G√©n√©ral | Reranking/L√©ger |

---

## üöÄ Prochaines √âtapes

1. ‚úÖ Code mis √† jour avec dual model support
2. ‚úÖ Documentation compl√®te
3. ‚è≥ **Test local** - V√©rifier les 2 mod√®les
4. ‚è≥ **Deployment Railway** - Push et test
5. ‚è≥ **Int√©gration N8N** - Cr√©er les 2 credentials
6. ‚è≥ **Test workflow** - V√©rifier les 2 mod√®les dans N8N
7. ‚è≥ **Plugin N8N rerank** - Cr√©er le node custom (plus tard)

---

## üìö R√©f√©rences

- **HuggingFace int8:** https://huggingface.co/C10X/int8
- **HuggingFace TurboX.v2:** https://huggingface.co/C10X/Qwen3-Embedding-TurboX.v2
- **Model2Vec:** https://github.com/MinishLab/model2vec
- **Config int8:** Tokenizer = Qwen/Qwen3-Reranker-0.6B, 256D output

---

*Int√©gration cr√©√©e: 2025-10-09*
*Service: Dual Model Embeddings (1024D + 256D)*
*Stack: FastAPI + Model2Vec + Docker*
