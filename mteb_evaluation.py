#!/usr/bin/env python3
"""
MTEB (Massive Text Embedding Benchmark) Evaluation for Qwen25-1024D

This script runs the official MTEB benchmark on our Qwen25-1024D Model2Vec.

MTEB tests across 58 datasets covering:
- Classification (Banking77, AmazonReviewsClassification, etc.)
- Clustering (ArXivClustering, StackExchangeClustering, etc.)
- Pair Classification (SprintDuplicateQuestions, TwitterSemEval2015, etc.)
- Reranking (AskUbuntuDupQuestions, StackOverflowDupQuestions, etc.)
- Retrieval (ArguAna, FiQA2018, NFCorpus, etc.)
- STS (Semantic Textual Similarity)
- Summarization

Full benchmark takes ~4-8 hours on CPU, ~1-2 hours on GPU.
"""

import argparse
from model2vec import StaticModel
import mteb
from pathlib import Path
import json
from datetime import datetime

def run_mteb_benchmark(
    model_path: str,
    output_dir: str = "mteb_results",
    tasks: list = None,
    languages: list = None
):
    """
    Run MTEB benchmark on Qwen25-1024D Model2Vec

    Args:
        model_path: Path to the model (local or HuggingFace)
        output_dir: Directory to save results
        tasks: List of specific tasks to run (None = all tasks)
        languages: List of languages to test (None = English only)
    """

    print("=" * 80)
    print("MTEB Evaluation - Qwen25-1024D Model2Vec")
    print("=" * 80)
    print(f"Model: {model_path}")
    print(f"Output: {output_dir}")
    print()

    # Load model
    print("Loading model...")
    model = StaticModel.from_pretrained(model_path)

    # Get embedding dimensions by testing
    test_embedding = model.encode(["test"], show_progress_bar=False)[0]
    dimensions = len(test_embedding)

    print(f"‚úÖ Model loaded! Dimensions: {dimensions}")
    print()

    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Define tasks to run
    if tasks is None:
        # Default: Run most important English tasks (faster, ~2-3 hours)
        tasks = [
            # Classification (7 tasks)
            "Banking77Classification",
            "AmazonReviewsClassification",
            "EmotionClassification",
            "ToxicConversationsClassification",

            # Clustering (11 tasks)
            "ArXivClusteringP2P",
            "StackExchangeClustering",
            "TwentyNewsgroupsClustering",

            # Pair Classification (3 tasks)
            "SprintDuplicateQuestions",
            "TwitterSemEval2015",

            # Reranking (4 tasks)
            "AskUbuntuDupQuestions",
            "StackOverflowDupQuestions",

            # Retrieval (15 tasks) - Most important!
            "ArguAna",
            "FiQA2018",
            "NFCorpus",
            "QuoraRetrieval",
            "SCIDOCS",
            "SciFact",
            "TRECCOVID",

            # STS (Semantic Textual Similarity) (17 tasks)
            "STS12",
            "STS13",
            "STS14",
            "STS15",
            "STS16",
            "STSBenchmark",
            "SICK-R",

            # Summarization (1 task)
            "SummEval",
        ]

    if languages is None:
        languages = ["en"]  # English only for faster testing

    print(f"Running {len(tasks)} tasks in {len(languages)} language(s)...")
    print()

    # Create MTEB evaluation
    evaluation = mteb.MTEB(tasks=mteb.get_tasks(tasks=tasks, languages=languages))

    # Run evaluation
    print("üöÄ Starting MTEB evaluation...")
    print("This may take 2-8 hours depending on hardware and number of tasks.")
    print()

    start_time = datetime.now()

    results = evaluation.run(
        model,
        output_folder=str(output_path),
        eval_splits=["test"],  # Use test split
        overwrite_results=False,  # Skip already computed results
    )

    end_time = datetime.now()
    duration = end_time - start_time

    print()
    print("=" * 80)
    print("‚úÖ MTEB Evaluation Complete!")
    print("=" * 80)
    print(f"Duration: {duration}")
    print(f"Results saved to: {output_path}")
    print()

    # Print summary
    print_results_summary(results, output_path)

    return results

def print_results_summary(results, output_path):
    """Print a summary of MTEB results"""

    print("üìä Results Summary:")
    print("-" * 80)

    # Aggregate scores by task type
    task_scores = {}

    for task_name, task_results in results.items():
        # Extract task type from name
        if "Classification" in task_name:
            task_type = "Classification"
        elif "Clustering" in task_name:
            task_type = "Clustering"
        elif "PairClassification" in task_name or "Duplicate" in task_name:
            task_type = "PairClassification"
        elif "Reranking" in task_name or "Dup" in task_name:
            task_type = "Reranking"
        elif "Retrieval" in task_name or any(x in task_name for x in ["ArguAna", "FiQA", "NFCorpus", "Quora", "SCIDOCS", "SciFact", "TREC"]):
            task_type = "Retrieval"
        elif "STS" in task_name or "SICK" in task_name:
            task_type = "STS"
        elif "Summ" in task_name:
            task_type = "Summarization"
        else:
            task_type = "Other"

        # Extract main score
        if isinstance(task_results, dict) and "test" in task_results:
            test_results = task_results["test"]
            # Try to find the main metric
            if "main_score" in test_results:
                score = test_results["main_score"]
            elif "ndcg_at_10" in test_results:
                score = test_results["ndcg_at_10"]
            elif "map" in test_results:
                score = test_results["map"]
            elif "accuracy" in test_results:
                score = test_results["accuracy"]
            elif "cosine_spearman" in test_results:
                score = test_results["cosine_spearman"]
            else:
                # Take first available numeric score
                score = next((v for v in test_results.values() if isinstance(v, (int, float))), None)

            if score is not None:
                if task_type not in task_scores:
                    task_scores[task_type] = []
                task_scores[task_type].append(score)
                print(f"  {task_name}: {score:.4f}")

    print()
    print("üìà Average Scores by Task Type:")
    print("-" * 80)

    overall_scores = []
    for task_type, scores in sorted(task_scores.items()):
        avg_score = sum(scores) / len(scores)
        overall_scores.extend(scores)
        print(f"  {task_type:20s}: {avg_score:.4f} ({len(scores)} tasks)")

    if overall_scores:
        overall_avg = sum(overall_scores) / len(overall_scores)
        print()
        print(f"üéØ OVERALL MTEB SCORE: {overall_avg:.4f}")

    print()
    print(f"Full results saved to: {output_path}")
    print()

def run_quick_mteb(model_path: str, output_dir: str = "mteb_results_quick"):
    """
    Run a quick MTEB evaluation with only the most important tasks (~30 min)
    """

    quick_tasks = [
        # 1 Classification task
        "Banking77Classification",

        # 1 Clustering task
        "TwentyNewsgroupsClustering",

        # 1 Pair Classification
        "SprintDuplicateQuestions",

        # 2 Retrieval tasks (most important)
        "NFCorpus",
        "SciFact",

        # 2 STS tasks
        "STSBenchmark",
        "SICK-R",
    ]

    print("üöÄ Running QUICK MTEB evaluation (7 tasks, ~30 minutes)...")
    return run_mteb_benchmark(model_path, output_dir, tasks=quick_tasks)

def run_full_mteb(model_path: str, output_dir: str = "mteb_results_full"):
    """
    Run the complete MTEB benchmark (58 tasks, 4-8 hours)
    """

    print("üöÄ Running FULL MTEB evaluation (58 tasks, 4-8 hours)...")
    return run_mteb_benchmark(model_path, output_dir, tasks=None)

def compare_with_baselines(results_path: str):
    """
    Compare results with known baseline models
    """

    print("üìä Comparison with Baseline Models:")
    print("-" * 80)

    baselines = {
        "text-embedding-ada-002": 60.99,
        "text-embedding-3-small": 62.26,
        "text-embedding-3-large": 64.59,
        "gte-large": 63.13,
        "e5-large-v2": 62.25,
        "instructor-xl": 61.79,
    }

    # Load our results
    results_file = Path(results_path) / "qwen25-deposium-1024d_results.json"
    if results_file.exists():
        with open(results_file) as f:
            our_results = json.load(f)

        # Calculate our average score
        # (This is simplified - real MTEB score is weighted)
        our_score = 0.0  # Extract from results

        print(f"Qwen25-1024D: {our_score:.2f}")
        print()

        for model, score in sorted(baselines.items(), key=lambda x: x[1], reverse=True):
            diff = our_score - score
            symbol = "+" if diff > 0 else ""
            print(f"{model:30s}: {score:.2f} ({symbol}{diff:.2f})")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run MTEB benchmark on Qwen25-1024D Model2Vec"
    )
    parser.add_argument(
        "--model",
        default="models/qwen25-deposium-1024d",
        help="Path to model (local or HuggingFace repo)"
    )
    parser.add_argument(
        "--output",
        default="mteb_results",
        help="Output directory for results"
    )
    parser.add_argument(
        "--mode",
        choices=["quick", "full", "custom"],
        default="quick",
        help="Evaluation mode: quick (7 tasks, ~30 min), full (58 tasks, 4-8 hours), custom"
    )
    parser.add_argument(
        "--tasks",
        nargs="+",
        help="List of specific tasks to run (for custom mode)"
    )

    args = parser.parse_args()

    if args.mode == "quick":
        results = run_quick_mteb(args.model, args.output)
    elif args.mode == "full":
        results = run_full_mteb(args.model, args.output)
    else:  # custom
        if not args.tasks:
            print("‚ùå Error: --tasks required for custom mode")
            exit(1)
        results = run_mteb_benchmark(args.model, args.output, tasks=args.tasks)

    print()
    print("üéâ MTEB evaluation completed!")
    print(f"Results: {args.output}")
