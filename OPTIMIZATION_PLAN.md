# EmbeddingGemma Optimization Plan

## Context

**Problem**: EmbeddingGemma-300m float32 (~1.2GB) est trop lent sur Railway CPU:
- 512 tokens: ~15ms/embedding
- 1024 tokens: ~40ms/embedding
- 2048 tokens: ~115ms/embedding

**Solution**: Strat√©gie multi-variantes pour diff√©rents use cases

## Variantes Propos√©es

### 1. gemma (baseline) ‚úÖ DEPLOYED
**Status**: Actuellement d√©ploy√©
**Taille**: 587MB (float16) / 1.2GB (float32)
**Performance CPU**: ~115ms/2048 tokens
**Qualit√©**: MTEB 0.80 Spearman
**Use case**: Baseline de r√©f√©rence, GPU local

**Probl√®me actuel**: Float16 CPU produit des z√©ros ‚Üí Float32 fallback automatique

### 2. gemma-int8 (PRIORIT√â 1) üéØ
**Taille cible**: ~300MB (-75%)
**Performance CPU estim√©e**: ~40ms/2048 tokens (3x plus rapide)
**Qualit√© estim√©e**: MTEB 0.78-0.79 (-1-2%)
**Use case**: Production Railway CPU

**Impl√©mentation**:
```python
from optimum.bettertransformer import BetterTransformer
from transformers import AutoModel
import torch

# Load with INT8 quantization
model = AutoModel.from_pretrained(
    "google/embeddinggemma-300m",
    load_in_8bit=True,  # INT8 quantization
    device_map="auto"
)
model = BetterTransformer.transform(model)  # Flash Attention optimizations
```

**D√©pendances additionnelles**:
```
optimum>=1.16.0
bitsandbytes>=0.41.0
accelerate>=0.25.0
```

**Avantages**:
- ‚úÖ Compatible sentence-transformers
- ‚úÖ Pas de retraining n√©cessaire
- ‚úÖ Perte de qualit√© minimale
- ‚úÖ Rapide √† d√©ployer (~1 jour)

### 3. gemma-onnx-int8 (PRIORIT√â 2) ‚ö°
**Taille cible**: ~200MB (-83%)
**Performance CPU estim√©e**: ~25ms/2048 tokens (5x plus rapide)
**Qualit√© estim√©e**: MTEB 0.78-0.79 (-1-2%)
**Use case**: Production Railway CPU (max performance)

**Impl√©mentation**:
```python
from optimum.onnxruntime import ORTModelForFeatureExtraction
from optimum.onnxruntime.configuration import OptimizationConfig, QuantizationConfig

# Convert to ONNX with INT8 quantization
model = ORTModelForFeatureExtraction.from_pretrained(
    "google/embeddinggemma-300m",
    export=True
)

# Apply INT8 quantization
quantization_config = QuantizationConfig(is_static=False, format="QDQ")
model.quantize(save_dir="./gemma-onnx-int8", quantization_config=quantization_config)
```

**D√©pendances additionnelles**:
```
onnxruntime>=1.17.0
optimum[onnxruntime]>=1.16.0
```

**Avantages**:
- ‚úÖ Maximum performance CPU
- ‚úÖ Taille optimale
- ‚úÖ Compatible avec sentence-transformers via ORTModel
- ‚ö†Ô∏è N√©cessite conversion (peut prendre quelques heures)

### 4. gemma-distilled (PRIORIT√â 3) üöÄ
**Taille cible**: ~30MB (-97%)
**Performance CPU estim√©e**: ~2ms/2048 tokens (50x plus rapide)
**Qualit√© estim√©e**: MTEB 0.68-0.72 (-8-12%)
**Use case**: Applications temps r√©el, volume √©lev√©

**Impl√©mentation (Model2Vec)**:
```python
from model2vec import distill_model
from sentence_transformers import SentenceTransformer

# Load teacher model
teacher = SentenceTransformer("google/embeddinggemma-300m")

# Distill to static embeddings
distilled_model = distill_model(
    teacher,
    vocabulary_size=256000,  # Reduced vocabulary
    embedding_dim=768,       # Keep same dimensions
    output_folder="./gemma-distilled"
)
```

**Avantages**:
- ‚úÖ Ultra rapide (pas de forward pass neural)
- ‚úÖ Ultra compact
- ‚úÖ Parfait pour embedding de vocabulaire fixe
- ‚ö†Ô∏è Perte de qualit√© significative pour contexte long

## Roadmap d'Impl√©mentation

### Phase 1: INT8 Quantization (1-2 jours)
1. Ajouter d√©pendances optimum + bitsandbytes
2. Impl√©menter chargement INT8 avec load_in_8bit=True
3. Tester performance et qualit√© sur MTEB subset
4. D√©ployer sur Railway
5. Comparer latence vs gemma baseline

**Crit√®res de succ√®s**:
- Latence < 50ms pour 2048 tokens sur Railway CPU
- MTEB score > 0.78
- Taille < 350MB

### Phase 2: ONNX Conversion (2-3 jours)
1. Installer optimum[onnxruntime]
2. Convertir gemma-int8 en ONNX
3. Optimiser avec QuantizationConfig
4. Benchmark performance
5. D√©ployer variante gemma-onnx-int8

**Crit√®res de succ√®s**:
- Latence < 30ms pour 2048 tokens sur Railway CPU
- MTEB score > 0.78
- Taille < 250MB

### Phase 3: Model2Vec Distillation (3-5 jours)
1. Collecter dataset repr√©sentatif (1M textes)
2. Distiller avec Model2Vec
3. √âvaluer sur MTEB
4. D√©ployer variante gemma-distilled

**Crit√®res de succ√®s**:
- Latence < 5ms pour 2048 tokens sur Railway CPU
- MTEB score > 0.68
- Taille < 50MB

## Configuration Multi-Variantes

### API Endpoints
```
POST /api/embed
{
  "model": "gemma",           // Baseline float16/float32
  "model": "gemma-int8",      // INT8 quantized (RECOMMAND√â RAILWAY)
  "model": "gemma-onnx",      // ONNX INT8 optimized
  "model": "gemma-distilled", // Model2Vec ultra-fast
  "input": "text to embed"
}
```

### Selection Logic
```python
# Automatic model selection based on environment
if os.getenv("RAILWAY_ENVIRONMENT"):
    default_model = "gemma-int8"  # Railway CPU
elif torch.cuda.is_available():
    default_model = "gemma"        # Local GPU
else:
    default_model = "gemma-int8"  # Local CPU
```

## Benchmarks Attendus

| Variant | Size | Latency (2048 tok, CPU) | MTEB Score | Use Case |
|---------|------|------------------------|------------|----------|
| gemma (float32) | 1.2GB | ~115ms | 0.80 | Baseline, GPU local |
| gemma (float16) | 587MB | ‚ö†Ô∏è zeros bug | 0.80 | GPU local |
| **gemma-int8** | ~300MB | **~40ms** | **0.78-0.79** | **Railway CPU ‚úÖ** |
| gemma-onnx-int8 | ~200MB | **~25ms** | **0.78-0.79** | Railway CPU (optimal) |
| gemma-distilled | ~30MB | **~2ms** | 0.68-0.72 | High-volume, real-time |

## Next Steps

1. ‚úÖ D√©ployer gemma baseline avec float16‚Üífloat32 fallback (DONE)
2. üéØ Impl√©menter gemma-int8 (START NOW)
3. ‚è≥ Convertir en ONNX INT8
4. ‚è≥ Distiller avec Model2Vec

## Questions √† R√©soudre

1. **Quality threshold**: Quel MTEB score minimum acceptable?
   - Pour production: 0.75+ recommand√©
   - Pour high-volume: 0.65+ acceptable

2. **Multi-model strategy**: Charger toutes les variantes ou lazy loading?
   - Proposition: Charger seulement gemma-int8 sur Railway
   - Local: Charger gemma + gemma-int8 pour comparaison

3. **Fallback strategy**: Si gemma-int8 √©choue, fallback vers?
   - Proposition: gemma-int8 ‚Üí turbov2 (toujours disponible)

4. **Cost/Performance tradeoff**: Railway CPU pricing?
   - Latence actuelle: ~115ms ‚Üí Risque de timeout
   - Latence cible: <50ms ‚Üí Production-ready
