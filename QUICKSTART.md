# ðŸš€ TurboX.v2 Quick Start

## What is this?

**Ultra-fast CPU embedding service** - 20-40x faster than Qwen3-Embedding on CPU-only servers.

- âš¡ **4-14ms latency** (vs 200-600ms)
- ðŸ’¾ **30MB model** (vs 639MB)
- ðŸ§  **50MB RAM** (vs 1.3GB VRAM)
- ðŸ”¢ **1024D embeddings**
- ðŸŽ¯ **Railway-ready**

## 5-Minute Local Test

```bash
# 1. Build image
docker build -t deposium-embeddings-turbov2 .

# 2. Run container
docker run -d \
  --name turbo-test \
  --network deposium-internal \
  -p 11435:11435 \
  deposium-embeddings-turbov2

# 3. Test it (wait 10s for model download)
curl http://localhost:11435/health
# â†’ {"status":"healthy"}

curl -X POST http://localhost:11435/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model":"turbov2","input":"test"}'
# â†’ {"model":"turbov2","embeddings":[[1024 dimensions...]]}
```

## Add to N8N (Local)

**Qwen Embedding Tool node:**
```
Base URL: http://turbo-test:11435
Model: turbov2
```

**Test workflow:**
1. Add node â†’ Tools â†’ Qwen Embedding Tool
2. Configure credentials (no auth)
3. Input: "semantic search test"
4. Execute â†’ Should return embedding in <50ms âœ…

## Deploy to Railway

```bash
# 1. Push to GitHub (already done!)
git remote -v
# â†’ https://github.com/theseedship/deposium_embeddings-turbov2

# 2. Deploy to Railway
railway login
railway init
railway up

# 3. Get Railway URL
railway status
# â†’ https://deposium-embeddings-turbov2.railway.app
```

**Railway config:**
- Port: 11435
- Health: `/health`
- CPU: 0.5-1 vCPU
- RAM: 256-512MB

## Production N8N Setup

**Update Qwen Embedding credentials:**
```
Old (Ollama):
  Base URL: http://deposium-ollama:11434
  Model: qwen3-embedding:0.6b
  â†’ 200-600ms latency

New (TurboX.v2):
  Base URL: https://your-service.railway.app
  Model: turbov2
  â†’ 5-50ms latency âš¡
```

## Performance Numbers

| Metric | Qwen3 (Ollama) | TurboX.v2 | Improvement |
|--------|---------------|-----------|-------------|
| **Latency** | 200-600ms | 4-14ms | **20-40x faster** |
| **Model Size** | 639MB | 30MB | **21x smaller** |
| **Memory** | 1.3GB VRAM | 50MB RAM | **26x lighter** |
| **Batch** | Sequential | Native | **10x faster** |

## Files Overview

```
deposium_embeddings-turbov2/
â”œâ”€â”€ Dockerfile                  # Railway-ready build
â”œâ”€â”€ requirements.txt            # Python deps (FastAPI, model2vec)
â”œâ”€â”€ src/main.py                # FastAPI service
â”œâ”€â”€ README.md                  # Full documentation
â”œâ”€â”€ DEPLOYMENT_GUIDE.md        # Railway deployment
â”œâ”€â”€ BENCHMARK_RESULTS.md       # Performance analysis
â””â”€â”€ benchmark.py               # Performance tests
```

## Next Steps

### Option 1: Test Locally First
```bash
# Compare with Qwen3 in N8N workflows
# Measure actual latency improvement
# Validate embedding quality for your use case
```

### Option 2: Deploy to Railway Now
```bash
railway up
# â†’ Get URL â†’ Update N8N credentials â†’ Done!
```

### Option 3: Both (Recommended)
```bash
# 1. Local test (done âœ…)
# 2. Deploy to Railway
# 3. A/B test for 24h
# 4. Full migration if satisfied
```

## Cost Savings

**Railway monthly cost:**
- Qwen3 + Ollama: $10-20 (1-2GB RAM, optional GPU)
- TurboX.v2: **$2-5** (256-512MB RAM, CPU only)
- **Savings: 50-75%** ðŸ’°

## API Endpoints

```bash
# Health check
GET /health
â†’ {"status":"healthy"}

# List models
GET /api/tags
â†’ {"models":[{"name":"turbov2",...}]}

# Generate embedding
POST /api/embed
{
  "model": "turbov2",
  "input": "text" | ["text1", "text2"]
}
â†’ {"model":"turbov2","embeddings":[[1024D]]}
```

## Troubleshooting

### Container won't start
```bash
docker logs turbo-test
# Check for: "âœ… Model loaded successfully!"
```

### High latency (>100ms)
```bash
# Ensure same network as N8N
docker network inspect deposium-internal
```

### Railway deployment failed
```bash
railway logs
# Common: Forgot to expose port 11435
```

## Links

- **GitHub**: https://github.com/theseedship/deposium_embeddings-turbov2
- **Model**: https://huggingface.co/C10X/Qwen3-Embedding-TurboX.v2
- **Docs**: See `DEPLOYMENT_GUIDE.md` and `BENCHMARK_RESULTS.md`

---

**Status:** âœ… Ready for production
**Last Update:** 2025-10-09
**Performance:** 20-40x faster, 50-75% cheaper
