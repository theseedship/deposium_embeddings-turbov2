fastapi==0.115.6
uvicorn==0.32.1
model2vec==0.3.0
pydantic==2.9.2
numpy==1.26.4
safetensors==0.4.5
cachetools==5.5.0
psutil==6.0.0

# PyTorch CPU-only version installed separately in Dockerfile to use CPU-only index
# This saves 2-3GB RAM by excluding CUDA libraries

# Transformer libraries
transformers>=4.50.0  # Qwen3 requires recent transformers (model type 'qwen3')
sentence-transformers==3.3.1

# GPU-related dependencies removed (using CPU-only mode)
# bitsandbytes and accelerate not needed for CPU inference

# Optimum + ONNX Runtime for optimized CPU inference
# Note: Using latest optimum to support recent transformers versions
optimum[onnxruntime]>=1.25.0
onnxruntime==1.20.1

# MTEB for model evaluation (optional, dev only)
# mteb==1.18.0
