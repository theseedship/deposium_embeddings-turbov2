# Core API packages
fastapi==0.121.1
uvicorn==0.38.0
python-multipart==0.0.22  # Security fix: CVE-2024-24762 (DoS vulnerabilities)
pydantic==2.12.4

# Data processing
numpy>=2.0.0  # PyTorch 2.8+ is compatible with NumPy 2.x
safetensors==0.6.2
Pillow==12.0.0  # Note: Wheels 3-5x larger than v11

# PyTorch (security: CVE-2025-3730, CVE-2025-2953 fixed in 2.8.0)
torch>=2.8.0

# Transformers ecosystem (security: 6 ReDoS CVEs fixed in 4.53.0)
transformers>=4.53.0  # Compatible with optimum 1.25.x + sentence-transformers 5.1.2
sentence-transformers==5.1.2  # Warning: v5.1+ raises error on unused kwargs in encode()
model2vec==0.7.0  # Numpy-only inference, quantization support

# Quantization packages (4-bit for Qwen3 reranker)
bitsandbytes==0.48.2  # Requires PyTorch 2.3+ (compatible with 2.6.0)
accelerate==1.10.0  # FSDP2, FP8 support

# ONNX Runtime for optimized CPU inference (VL classifier)
# Note: Keeping optimum 1.25.x for stability (v2.0 has major breaking changes in ONNX integration)
optimum[onnxruntime]>=1.25.0
onnxruntime>=1.23.0  # v1.23+ supports NumPy 2.x

# MTEB for model evaluation (optional, dev only)
# mteb==1.18.0

# OpenBench for standardized LLM benchmarking
# Categories: Knowledge, Coding, Math, Reasoning, Cybersecurity, Search
# Docs: https://openbench.dev/installation
openbench>=0.1.0

# Mixedbread Reranker (SOTA cross-encoder, 100+ languages)
# Models: mxbai-rerank-base-v2 (0.5B), mxbai-rerank-large-v2 (2B)
# Docs: https://github.com/mixedbread-ai/mxbai-rerank
mxbai-rerank>=0.1.0

# HTTP client for remote backends (used by FastAPI internally)
httpx>=0.27.0

# =============================================================================
# Optional: High-Performance Inference Backends
# =============================================================================

# vLLM - High-throughput LLM inference (24x faster than HuggingFace)
# Requires: CUDA 12.1+, 8GB+ VRAM, Linux only
# Install: pip install vllm>=0.6.0
# Docs: https://docs.vllm.ai/
# vllm>=0.6.0

# OpenAI SDK - For remote OpenAI-compatible backends (vLLM server, SGLang, cloud)
# Install: pip install openai>=1.50.0
# openai>=1.50.0

