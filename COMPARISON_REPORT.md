# LEAF Model Comparison: v1 vs v2
**Comparison Date**: 2025-10-12
**Evaluation**: MTEB + Speed Benchmarks

---
## ðŸ“ Architecture Comparison
| Property | v1 (512 tokens) | v2 (2048 tokens) | Change |
|----------|-----------------|------------------|--------|
| **Layers** | 6 | 12 | +100% âœ… |
| **Parameters** | 75M | 120M | +60% âœ… |
| **Context Length** | 512 | 2048 | +300% âœ… |
| **Hidden Size Ratio** | 0.5x | 0.75x | +50% âœ… |
| **Training Data** | 50k samples | 200k samples | +300% âœ… |
| **Training Epochs** | 3 | 10 | +233% âœ… |
| **Alignment Loss Weight** | 1.0 | 2.5 | +150% âœ… |

## âš¡ Speed Comparison
| Metric | v1 | v2 | Change |
|--------|----|----|--------|
| **Throughput** | 0.0 texts/s | 0.0 texts/s | +0.0% |
| **Latency** | 0.00 ms | 0.00 ms | +0.0% |
| **Embedding Dims** | 768 | 768 | Same |

## ðŸ“Š Quality Comparison (MTEB)
### Known Results (v1)

| Task | Metric | v1 (FAILED) | v2 (Target) | Target Improvement |
|------|--------|-------------|-------------| -------------------|
| **STSBenchmark** | Spearman | 0.223 | 0.70+ | **+214%** ðŸŽ¯ |
| **STS22 English** | Spearman | 0.373 | 0.65+ | **+74%** ðŸŽ¯ |
| **STS22 Average** | Spearman | ~0.21 | 0.50+ | **+138%** ðŸŽ¯ |
| **Cross-lingual** | Spearman | -0.14 | 0.30+ | **Complete Fix** ðŸŽ¯ |
| **MTEB Score (est.)** | Overall | ~25 | 55+ | **+120%** ðŸŽ¯ |

## âŒ Detailed v1 Results (FAILED)

### STSBenchmark
- **Spearman**: 0.223 (Target: 0.81)
- **Quality Loss**: -72% vs base model
- **Status**: âŒ CRITICAL FAILURE

### STS22 by Language
| Language | Spearman | Status |
|----------|----------|--------|
| ðŸ‡¨ðŸ‡³ Chinese | 0.499 | ðŸŸ¡ Best (still poor) |
| ðŸ‡¸ðŸ‡¦ Arabic | 0.469 | ðŸŸ¡ Moderate |
| ðŸ‡®ðŸ‡¹ Italian | 0.435 | ðŸŸ¡ Moderate |
| ðŸ‡ªðŸ‡¸ Spanish | 0.403 | ðŸŸ  Poor |
| ðŸ‡¬ðŸ‡§ English | 0.373 | ðŸŸ  Poor |
| ðŸ‡«ðŸ‡· French | 0.300 | ðŸ”´ Very poor |
| ðŸ‡·ðŸ‡º Russian | 0.268 | ðŸ”´ Very poor |
| ðŸ‡¹ðŸ‡· Turkish | 0.247 | ðŸ”´ Very poor |
| ðŸ‡©ðŸ‡ª German | 0.163 | âŒ Critical |
| ðŸ‡µðŸ‡± Polish | 0.132 | âŒ Critical |

### Cross-lingual (Translation Tasks)
| Pair | Spearman | Status |
|------|----------|--------|
| ðŸ‡ªðŸ‡¸-ðŸ‡®ðŸ‡¹ | 0.119 | âŒ Failed |
| ðŸ‡©ðŸ‡ª-ðŸ‡µðŸ‡± | 0.113 | âŒ Failed |
| ðŸ‡©ðŸ‡ª-ðŸ‡«ðŸ‡· | 0.070 | âŒ Failed |
| ðŸ‡ªðŸ‡¸-ðŸ‡¬ðŸ‡§ | 0.002 | âŒ Random |
| ðŸ‡¨ðŸ‡³-ðŸ‡¬ðŸ‡§ | -0.012 | âŒ Inverse |
| ðŸ‡µðŸ‡±-ðŸ‡¬ðŸ‡§ | -0.143 | âŒ **WORST** |

## ðŸ“ Summary

### v1 (512 tokens) - FAILED
- âŒ **Architecture too aggressive**: 6 layers insufficient
- âŒ **Data insufficient**: 50k samples, mostly English
- âŒ **High alignment loss**: 2.18 (warning sign)
- âŒ **Quality catastrophic**: -72% vs base model
- âŒ **Multilingual destroyed**: Cross-lingual scores negative
- âœ… **Speed excellent**: 695 texts/s

### v2 (2048 tokens) - Expected Improvements
- âœ… **Architecture doubled**: 12 layers (2x)
- âœ… **Data 4x larger**: 200k multilingual samples
- âœ… **Alignment prioritized**: Weight 2.5 (vs 1.0)
- âœ… **Curriculum learning**: 512â†’1024â†’2048 progressive
- âœ… **Quality monitoring**: MTEB validation every 1000 steps
- âœ… **Target**: MTEB 55+ (vs ~25 in v1)
- âš ï¸ **Speed trade-off**: Likely ~400-500 texts/s (still fast)

## ðŸŽ¯ Recommendations

1. **Proceed with v2 training** using the improved configuration
2. **Monitor alignment loss** - stop if > 1.5 after epoch 3
3. **Validate frequently** - MTEB STSBenchmark every 1000 steps
4. **Target quality** - Spearman 0.70+ on STSBenchmark
5. **Expected training time** - 12-15 hours on RTX 4050

