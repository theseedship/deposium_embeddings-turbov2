# Qwen2.5-7B-Instruct â†’ Model2Vec Distillation Project

**ğŸ¯ Goal:** Distill Qwen/Qwen2.5-7B-Instruct to 65MB Model2Vec achieving 91-95% quality

**âš¡ Priority:** ABSOLUTE
**ğŸ“… Status:** Ready to start distillation
**â±ï¸ ETA:** 2-4 hours (GPU) or 10-20 hours (CPU)

---

## ğŸ“¦ What's Included

### Core Scripts (5 files)
1. **distill_qwen25_7b.py** (5.0KB)
   - Main distillation logic
   - Model2Vec conversion
   - Quality checks and metadata

2. **test_qwen25_7b_model.py** (5.7KB)
   - Basic encoding tests
   - Semantic similarity checks
   - Instruction awareness validation
   - Code understanding tests
   - Multilingual support verification

3. **quick_eval_qwen25_7b_1024d.py** (13KB)
   - Comprehensive evaluation suite
   - 6 category scores
   - Baseline comparison
   - Target validation

4. **Automation Scripts** (4 files)
   - `run_qwen25_7b_distillation.sh` (3.0KB) - Automated pipeline
   - `test_qwen25_7b_model.sh` (970B) - Quick test
   - `evaluate_qwen25_7b.sh` (1.1KB) - Automated eval
   - `deploy_qwen25_7b.sh` (8.7KB) - Production deployment

### Documentation (2 files)
5. **QWEN25_7B_DISTILLATION_GUIDE.md** (7.8KB)
   - Complete reference guide
   - Configuration options
   - Troubleshooting
   - Performance tuning

6. **QWEN25_7B_QUICKSTART.md** (4.6KB)
   - Fast-track instructions
   - 3-step process
   - Success indicators

**Total:** 9 files, 48.9KB documentation & scripts

---

## ğŸš€ Quick Start (3 Steps)

### Step 1: Distill (2-4 hours)
```bash
./run_qwen25_7b_distillation.sh
```

### Step 2: Test (2 minutes)
```bash
./test_qwen25_7b_model.sh
```

### Step 3: Evaluate (5 minutes)
```bash
./evaluate_qwen25_7b.sh
```

**If score â‰¥ 91%:** Deploy with `./deploy_qwen25_7b.sh`

---

## ğŸ¯ Expected Results

### Quality Targets
| Metric | Target | Baseline | Improvement |
|--------|--------|----------|-------------|
| **Overall** | **91-95%** | 68.2% | **+23-27%** |
| Instruction Awareness | 96-98% | 95.3% | +1-3% |
| Semantic Similarity | 96-98% | 95.0% | +1-3% |
| Code Understanding | 92-96% | 86.4% | +6-10% |
| Domain Knowledge | 88-92% | 65-70% | +18-25% |
| Multilingual | 85-90% | 60-65% | +20-28% |

### Model Specifications
- **Size:** ~65MB (vs 14GB full model)
- **Dimensions:** 1024D
- **Vocabulary:** 32K tokens (Qwen tokenizer)
- **Speed:** 500-1000x faster than full model
- **Latency:** <1ms per query
- **Memory:** <512MB runtime

---

## ğŸ“‹ Prerequisites

### Hardware
- **Minimum:** 32GB RAM, 50GB disk, CPU
- **Recommended:** 32GB RAM, 50GB disk, GPU 16GB+ VRAM

### Software
```bash
# Python 3.10+
python3 --version

# Virtual environment
python3 -m venv venv
source venv/bin/activate

# Dependencies
pip install -r requirements.txt
```

### Check GPU (optional but recommended)
```bash
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
nvidia-smi  # Should show GPU details
```

---

## ğŸ“Š Timeline

| Step | GPU Time | CPU Time | Description |
|------|----------|----------|-------------|
| 1. Distillation | 2-4 hours | 10-20 hours | Main process |
| 2. Testing | 2 minutes | 2 minutes | Sanity checks |
| 3. Evaluation | 5 minutes | 5 minutes | Quality metrics |
| 4. Deployment | 10 minutes | 10 minutes | Docker build |
| **Total** | **2-4 hours** | **10-20 hours** | **Complete pipeline** |

---

## ğŸ† Success Criteria

**âœ… Ready for production if:**
- Overall quality â‰¥ 91%
- Instruction awareness â‰¥ 95%
- Code understanding â‰¥ 90%
- Model size â‰¤ 70MB
- All tests pass
- Docker container runs successfully

**âš ï¸ Re-distill if:**
- Overall quality < 88%
- Tests fail
- Model size > 80MB

---

## ğŸ“š Documentation Structure

```
QWEN25_7B_README.md                    â† You are here (overview)
QWEN25_7B_QUICKSTART.md                â† Fast-track (3 steps)
QWEN25_7B_DISTILLATION_GUIDE.md        â† Complete reference
```

**Which to read?**
- **In a hurry?** â†’ `QWEN25_7B_QUICKSTART.md`
- **Want details?** â†’ `QWEN25_7B_DISTILLATION_GUIDE.md`
- **Just getting started?** â†’ This file

---

## ğŸ”§ Customization

### Change Dimensions
Edit `distill_qwen25_7b.py`:
```python
CONFIG = {
    "pca_dims": 1536,  # Increase for higher quality
}
```

### Use Larger Corpus
```python
CONFIG = {
    "corpus_size": 2_000_000,  # Increase for better quality
}
```

### Faster Distillation
```python
CONFIG = {
    "pca_dims": 768,  # Lower dimensions = faster
    "corpus_size": 500_000,  # Smaller corpus = faster
}
```

---

## ğŸ†˜ Troubleshooting

### Out of Memory
```bash
# Force CPU mode
export CUDA_VISIBLE_DEVICES=""
./run_qwen25_7b_distillation.sh
```

### Slow Progress
- **Normal on CPU:** 10-20 hours expected
- **Use GPU:** 10x faster (2-4 hours)

### Low Quality Score
1. Check model path
2. Re-run with better parameters
3. Compare with baseline

See `QWEN25_7B_DISTILLATION_GUIDE.md` for detailed troubleshooting.

---

## ğŸ“¦ Deployment

After successful evaluation (â‰¥ 91%):

```bash
# Automated deployment
./deploy_qwen25_7b.sh

# Manual deployment
docker run -p 8080:8080 deposium-embeddings-v11:latest

# Production push
docker tag deposium-embeddings-v11:latest your-registry/deposium:v11
docker push your-registry/deposium:v11
```

---

## ğŸ”„ Project Workflow

```
1. Preparation (Done âœ…)
   â”œâ”€â”€ Scripts created
   â”œâ”€â”€ Documentation written
   â””â”€â”€ Configuration set

2. Distillation (Next â³)
   â”œâ”€â”€ Download Qwen2.5-7B (14GB)
   â”œâ”€â”€ Distill to Model2Vec
   â””â”€â”€ Save to models/ (65MB)

3. Validation (After distillation)
   â”œâ”€â”€ Run tests
   â”œâ”€â”€ Run evaluation
   â””â”€â”€ Check score â‰¥ 91%

4. Deployment (If successful)
   â”œâ”€â”€ Update API
   â”œâ”€â”€ Build Docker
   â”œâ”€â”€ Test container
   â””â”€â”€ Deploy to production

5. Documentation (Final step)
   â”œâ”€â”€ Update README
   â”œâ”€â”€ Add benchmarks
   â””â”€â”€ Create deployment summary
```

---

## ğŸ¯ Why Qwen2.5-7B?

### SOTA Performance
- MMLU: 83.5% (general knowledge)
- GSM8K: 93.6% (math reasoning)
- HumanEval: 89.5% (code generation)

### Best-in-Class Features
- âœ… Multilingual (29+ languages)
- âœ… Code-aware (massive code corpus)
- âœ… Instruction-tuned (excellent for RAG)
- âœ… Long context (128K tokens)
- âœ… Efficient (beats GPT-3.5 at 7B)

### Model2Vec Benefits
- âš¡ 500-1000x faster inference
- ğŸ“¦ 215x smaller (65MB vs 14GB)
- ğŸ’° 10-100x cheaper compute
- ğŸ”‹ Edge-deployable

---

## ğŸ“ Support & Resources

### Documentation
- This README: Overview and quick reference
- Quickstart: `QWEN25_7B_QUICKSTART.md`
- Full guide: `QWEN25_7B_DISTILLATION_GUIDE.md`

### External Resources
- Model2Vec: https://github.com/MinishLab/model2vec
- Qwen2.5: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
- HuggingFace: https://huggingface.co/docs

### Scripts
- All scripts are self-documented with comments
- Use `python3 script.py --help` for usage
- Check script header for description

---

## âœ… Current Status

**Preparation:** âœ… Complete
**Configuration:** âœ… Ready
**Next Step:** ğŸš€ Run distillation

**To start:**
```bash
./run_qwen25_7b_distillation.sh
```

---

**Last Updated:** 2025-10-14
**Priority:** ğŸ”¥ ABSOLUTE
**Target:** 91-95% quality in 2-4 hours
**Status:** âœ… Ready to launch
