# ONNX INT8 Model - Success! üéâ

**Date**: 2025-10-12
**Status**: Model loading successful, ready for benchmarking

---

## Summary

Successfully downloaded and tested the official ONNX INT8 model from `onnx-community/embeddinggemma-300m-ONNX`.

---

## What Worked

### Download
- **Source**: `onnx-community/embeddinggemma-300m-ONNX` (official HuggingFace ONNX export)
- **Model File**: `model_quantized.onnx` + `model_quantized.onnx_data`
- **Size**: 294.6 MB (INT8 quantized)
- **Total Package**: 2.6 GB (includes FP32, FP16, INT4, INT8 versions)

### Test Results
- ‚úÖ Model loads successfully with Optimum `ORTModelForFeatureExtraction`
- ‚úÖ Tokenizer loads correctly
- ‚úÖ Embeddings generated (768D vectors)
- ‚úÖ No NaN/Inf values detected
- ‚úÖ Non-zero embeddings
- ‚úÖ Cosine similarity: 0.6105 (reasonable for related texts)

---

## What Didn't Work

### Attempted: electroglyph/embeddinggemma-300m-ONNX-uint8
- **Issue**: Output structure incompatible with Optimum API
- **Error**: `KeyError: 'last_hidden_state'`
- **Reason**: Model exported with different output structure, not compatible with `ORTModelForFeatureExtraction`

### Attempted: Manual ONNX conversion with Optimum
- **Issue**: Gemma3 architecture not natively supported by Optimum
- **Error**: "Trying to export a gemma3_text model, that is a custom or unsupported architecture"
- **Reason**: Gemma3 is too new, requires custom ONNX configuration

---

## Next Steps

### 1. Benchmark Performance ‚è≥
Run `benchmark_onnx.py` to measure:
- Latency comparison: PyTorch INT8 vs ONNX INT8
- Expected: 3-4x speedup
- Target: <15ms per embedding locally

### 2. Railway Deployment
If local benchmarks show good performance:
1. Update `main.py` to support ONNX model
2. Update `Dockerfile` to include ONNX model
3. Deploy to Railway
4. Measure actual Railway performance

### 3. Success Criteria
- ‚úÖ Local latency: <15ms per embedding
- ‚è≥ MTEB score: >0.77 (<2% loss from 0.788 baseline)
- ‚è≥ Railway performance: <5s per paragraph

---

## Files Created
- `download_onnx_model.py` - Download script (updated to use onnx-community)
- `test_onnx_model.py` - Model validation (updated for subdirectory structure)
- `check_onnx_outputs.py` - Output structure debugging
- `models/gemma-onnx-int8/` - Downloaded ONNX models
- `onnx_download.log` - Download logs
- `onnx_test.log` - Test results

---

## Available Model Variants
The onnx-community package includes multiple quantized versions:
- `model.onnx` (1177 MB) - FP32 baseline
- `model_fp16.onnx` (589 MB) - FP16
- **`model_quantized.onnx` (295 MB) - INT8 ‚Üê Currently using**
- `model_q4.onnx` (188 MB) - INT4 (aggressive quantization)
- `model_q4f16.onnx` (167 MB) - INT4 + FP16 activations

If INT8 performance is insufficient, we can try INT4 for additional speedup (with potential quality tradeoff).

---

**Last Updated**: 2025-10-12 21:00
**Status**: Ready for benchmarking
