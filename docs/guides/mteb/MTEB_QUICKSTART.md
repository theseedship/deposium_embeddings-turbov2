# MTEB Evaluation - Quick Start

√âvalue ton mod√®le Qwen25-1024D avec le benchmark MTEB officiel.

## üöÄ 3 Options d'√âvaluation

### Option 1: Quick Test (30 min) - RECOMMAND√â POUR COMMENCER

```bash
./run_mteb_quick.sh
```

**7 t√¢ches repr√©sentatives** pour avoir une premi√®re id√©e des performances.

### Option 2: Retrieval Only (1-2h) - POUR RAG/SEARCH

```bash
./run_mteb_retrieval.sh
```

**15 t√¢ches de retrieval** - les plus importantes pour RAG, semantic search, Q&A.

### Option 3: Full Benchmark (4-8h) - OFFICIEL

```bash
./run_mteb_full.sh
```

**58 t√¢ches compl√®tes** - score MTEB officiel pour publication.

## üìä R√©sultats Attendus

**Qwen25-1024D vs Full-size Models:**

| Mod√®le | MTEB Score | Taille | Latence | Use Case |
|--------|------------|--------|---------|----------|
| text-embedding-3-large | 64.59 | ~1GB | 50-200ms | Maximum qualit√© |
| gte-large | 63.13 | 670MB | 30-100ms | Haute qualit√© |
| **Qwen25-1024D** | **~45-55** | **65MB** | **<10ms** | **Speed + Efficiency** |

**Trade-off:** -10 √† -15 points MTEB mais **500-1000x plus rapide!**

## üéØ Pourquoi c'est important?

MTEB est le **benchmark de r√©f√©rence** pour les embeddings:
- Utilis√© par OpenAI, Cohere, HuggingFace
- 58 datasets, 8 types de t√¢ches
- Score comparable entre tous les mod√®les

## üìÅ R√©sultats

Apr√®s l'√©valuation:

```bash
# Voir le r√©sum√©
cat mteb_results_quick/qwen25-deposium-1024d_results.json | python3 -m json.tool | head -50

# Calculer le score moyen
python3 -c "
import json
data = json.load(open('mteb_results_quick/qwen25-deposium-1024d_results.json'))
scores = [v['test']['main_score'] for v in data.values() if 'test' in v]
print(f'Average MTEB Score: {sum(scores)/len(scores):.4f}')
"
```

## üìö Guide Complet

Pour plus de d√©tails: `MTEB_GUIDE.md`

---

**Recommandation:** Commence par `./run_mteb_quick.sh` pour avoir un premier score en 30 minutes! üöÄ
