# Guide d'Ã‰valuation MTEB pour Qwen25-1024D

Guide complet pour Ã©valuer Qwen25-1024D avec le benchmark MTEB officiel.

---

## ğŸ“Š Qu'est-ce que MTEB?

**MTEB (Massive Text Embedding Benchmark)** est le benchmark de rÃ©fÃ©rence pour Ã©valuer les modÃ¨les d'embeddings.

### Couverture
- **58 datasets** couvrant 8 types de tÃ¢ches
- **112 langues** supportÃ©es
- **~10M+ exemples** de test

### Types de tÃ¢ches
1. **Classification** (7 datasets) - Banking77, Amazon Reviews, etc.
2. **Clustering** (11 datasets) - ArXiv, StackExchange, etc.
3. **Pair Classification** (3 datasets) - Duplicate questions
4. **Reranking** (4 datasets) - Question duplicates
5. **Retrieval** (15 datasets) - ArguAna, FiQA, NFCorpus, Quora, etc. â­ **PLUS IMPORTANT**
6. **STS** (17 datasets) - Semantic Textual Similarity
7. **Summarization** (1 dataset) - SummEval
8. **Bittext Mining** - Translation pair mining

---

## ğŸš€ Installation

### Option 1: Environnement virtuel (recommandÃ©)

```bash
# CrÃ©er un venv dÃ©diÃ© pour MTEB
python3 -m venv venv_mteb
source venv_mteb/bin/activate

# Installer les dÃ©pendances
pip install -r requirements_mteb.txt
```

### Option 2: Utiliser l'environnement Qwen25 existant

```bash
source venv_qwen25/bin/activate
pip install mteb datasets
```

---

## ğŸ“‹ Modes d'Ã‰valuation

### Mode 1: Quick Test (recommandÃ© pour dÃ©buter)

**DurÃ©e:** ~30 minutes
**Tasks:** 7 tÃ¢ches reprÃ©sentatives

```bash
python3 mteb_evaluation.py --mode quick
```

Teste:
- 1 Classification task
- 1 Clustering task
- 1 Pair Classification
- 2 Retrieval tasks (NFCorpus, SciFact)
- 2 STS tasks (STSBenchmark, SICK-R)

### Mode 2: Standard Test (balance temps/couverture)

**DurÃ©e:** ~2-3 heures
**Tasks:** ~20 tÃ¢ches essentielles

```bash
python3 mteb_evaluation.py --mode custom --tasks \
  Banking77Classification \
  ArXivClusteringP2P \
  SprintDuplicateQuestions \
  AskUbuntuDupQuestions \
  ArguAna FiQA2018 NFCorpus QuoraRetrieval SCIDOCS SciFact TRECCOVID \
  STS12 STS13 STS14 STS15 STS16 STSBenchmark SICK-R \
  SummEval
```

### Mode 3: Full MTEB Benchmark (officiel)

**DurÃ©e:** ~4-8 heures (CPU) ou ~1-2 heures (GPU)
**Tasks:** 58 tÃ¢ches complÃ¨tes

```bash
python3 mteb_evaluation.py --mode full
```

---

## ğŸ¯ ExÃ©cution

### Test Rapide (Quick Mode)

```bash
# Activer l'environnement
source venv_mteb/bin/activate

# Lancer le test rapide
python3 mteb_evaluation.py \
  --model models/qwen25-deposium-1024d \
  --output mteb_results_quick \
  --mode quick
```

### Test Complet

```bash
# Sur GPU (si disponible) - BEAUCOUP plus rapide!
CUDA_VISIBLE_DEVICES=0 python3 mteb_evaluation.py \
  --model models/qwen25-deposium-1024d \
  --output mteb_results_full \
  --mode full

# Sur CPU (plus lent mais fonctionne)
python3 mteb_evaluation.py \
  --model models/qwen25-deposium-1024d \
  --output mteb_results_full \
  --mode full
```

### Test PersonnalisÃ©

```bash
# Tester uniquement les tÃ¢ches de Retrieval (les plus importantes)
python3 mteb_evaluation.py \
  --mode custom \
  --tasks ArguAna FiQA2018 NFCorpus QuoraRetrieval SCIDOCS SciFact TRECCOVID
```

---

## ğŸ“Š RÃ©sultats Attendus

### Comparaison avec ModÃ¨les de RÃ©fÃ©rence

**ModÃ¨les Full-size (baseline):**
```
Model                          MTEB Score   Size      Speed
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
text-embedding-3-large         64.59        ~1GB      Slow (API)
gte-large                      63.13        670MB     Medium
text-embedding-3-small         62.26        ~350MB    Medium
e5-large-v2                    62.25        1.34GB    Slow
instructor-xl                  61.79        4.96GB    Very Slow
text-embedding-ada-002         60.99        ~350MB    Medium (API)
```

**Qwen25-1024D (notre modÃ¨le):**
```
Model                          MTEB Score   Size      Speed
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Qwen25-1024D Model2Vec         ~45-55*      65MB      500-1000x FASTER!
```

*Note: Score estimÃ© - Model2Vec sacrifie ~10-15 points MTEB pour gagner 500-1000x en vitesse*

### Pourquoi le score sera plus bas?

**Trade-offs de Model2Vec:**
- âŒ Score MTEB: ~45-55 (vs ~62-65 pour full-size)
- âœ… Taille: 65MB (vs 350MB-5GB)
- âœ… Vitesse: 500-1000x plus rapide
- âœ… Latence: <10ms (vs 50-500ms)
- âœ… MÃ©moire: ~100MB RAM (vs 1-8GB)
- âœ… CoÃ»t: Gratuit local (vs API payant)

### OÃ¹ Qwen25-1024D excelle

**Forces attendues:**
1. **STS (Semantic Textual Similarity)** - Score Ã©levÃ© attendu (~60-70)
2. **Classification simple** - TrÃ¨s bon (~55-65)
3. **Clustering** - Bon (~50-60)

**Faiblesses attendues:**
1. **Retrieval complexe** - Plus faible (~40-50) mais acceptable
2. **Long documents** - LimitÃ© par le tokenizer

---

## ğŸ“ Structure des RÃ©sultats

```
mteb_results/
â”œâ”€â”€ qwen25-deposium-1024d_results.json          # RÃ©sultats complets JSON
â”œâ”€â”€ Banking77Classification/
â”‚   â””â”€â”€ test_results.json                       # RÃ©sultats par task
â”œâ”€â”€ STSBenchmark/
â”‚   â””â”€â”€ test_results.json
â”œâ”€â”€ NFCorpus/
â”‚   â””â”€â”€ test_results.json
â””â”€â”€ ...
```

### Format des rÃ©sultats (JSON)

```json
{
  "Banking77Classification": {
    "test": {
      "accuracy": 0.8234,
      "f1": 0.8156,
      "main_score": 0.8234
    }
  },
  "STSBenchmark": {
    "test": {
      "cosine_pearson": 0.7856,
      "cosine_spearman": 0.7823,
      "main_score": 0.7823
    }
  }
}
```

---

## ğŸ”¬ Analyse des RÃ©sultats

### Commandes Utiles

```bash
# Voir le rÃ©sumÃ©
cat mteb_results_quick/qwen25-deposium-1024d_results.json | python3 -m json.tool | head -50

# Extraire le score moyen
python3 -c "
import json
with open('mteb_results_quick/qwen25-deposium-1024d_results.json') as f:
    data = json.load(f)
    scores = [v['test']['main_score'] for v in data.values() if 'test' in v]
    print(f'Average MTEB Score: {sum(scores)/len(scores):.4f}')
"

# Comparer avec baseline
python3 mteb_evaluation.py --compare mteb_results_full
```

---

## âš¡ Optimisation

### Pour GPU (Tesla T4, A100, etc.)

```bash
# Utiliser GPU
export CUDA_VISIBLE_DEVICES=0

# Augmenter batch size (Model2Vec est trÃ¨s rapide)
# Note: Model2Vec n'a pas de batch processing natif, mais MTEB le gÃ¨re
python3 mteb_evaluation.py --mode full
```

### Pour CPU multi-core

```bash
# Utiliser tous les cores
export OMP_NUM_THREADS=16
export MKL_NUM_THREADS=16

python3 mteb_evaluation.py --mode full
```

---

## ğŸ“Š Publier les RÃ©sultats

### Sur HuggingFace Hub

```bash
# Upload results to model card
python3 -c "
from huggingface_hub import HfApi
api = HfApi()

api.upload_file(
    path_or_fileobj='mteb_results_full/qwen25-deposium-1024d_results.json',
    path_in_repo='mteb_results.json',
    repo_id='tss-deposium/qwen25-deposium-1024d',
    repo_type='model'
)
"
```

### Mettre Ã  jour le README

Ajouter Ã  `models/qwen25-deposium-1024d/README.md`:

```markdown
## MTEB Benchmark Results

| Task Type | Score | # Tasks |
|-----------|-------|---------|
| Classification | 0.XXX | 7 |
| Clustering | 0.XXX | 11 |
| PairClassification | 0.XXX | 3 |
| Reranking | 0.XXX | 4 |
| Retrieval | 0.XXX | 15 |
| STS | 0.XXX | 17 |
| Summarization | 0.XXX | 1 |
| **Overall** | **0.XXX** | **58** |

**Comparison:**
- Full-size models: ~62-65 MTEB score, 350MB-5GB, 50-500ms latency
- Qwen25-1024D: ~XX MTEB score, 65MB, <10ms latency (**500-1000x faster!**)
```

---

## ğŸ› Troubleshooting

### Erreur: Out of Memory

```bash
# RÃ©duire le nombre de tasks simultanÃ©es
# Lancer task par task
for task in Banking77Classification STSBenchmark NFCorpus; do
    python3 mteb_evaluation.py --mode custom --tasks $task
done
```

### Erreur: Dataset Download Failed

```bash
# PrÃ©charger les datasets
python3 -c "
from mteb import MTEB
tasks = MTEB(tasks=['Banking77Classification'])
tasks.run(None, output_folder='test', eval_splits=['test'])
"
```

### Trop Lent?

```bash
# Mode quick seulement (30 min au lieu de 4-8h)
python3 mteb_evaluation.py --mode quick

# Ou tester 1 seule task pour validation
python3 mteb_evaluation.py --mode custom --tasks STSBenchmark
```

---

## ğŸ“š Ressources

- **MTEB Leaderboard:** https://huggingface.co/spaces/mteb/leaderboard
- **MTEB Paper:** https://arxiv.org/abs/2210.07316
- **MTEB GitHub:** https://github.com/embeddings-benchmark/mteb
- **Documentation:** https://github.com/embeddings-benchmark/mteb/tree/main/docs

---

## ğŸ¯ Checklist d'Ã‰valuation

- [ ] Environnement MTEB installÃ© (`venv_mteb`)
- [ ] ModÃ¨le Qwen25-1024D disponible localement
- [ ] Test rapide (quick mode) exÃ©cutÃ© (~30 min)
- [ ] RÃ©sultats quick analysÃ©s
- [ ] Test complet (full mode) lancÃ© (~4-8h)
- [ ] RÃ©sultats publiÃ©s sur HuggingFace
- [ ] README mis Ã  jour avec scores MTEB
- [ ] Comparaison avec baselines documentÃ©e

---

**PrÃªt pour l'Ã©valuation MTEB! ğŸš€**

Commence avec `python3 mteb_evaluation.py --mode quick` pour un test rapide.
