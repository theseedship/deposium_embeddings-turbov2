# Distillation Qwen2.5-7B â†’ Model2Vec - Guide Complet

**ğŸ¯ Objectif:** Convertir Qwen/Qwen2.5-7B-Instruct (14GB) en Model2Vec 1024D (~65MB)
**ğŸ“Š Performance cible:** 91-95% (+7-11% vs baseline Qwen2.5-1.5B)
**âš¡ PrioritÃ©:** ABSOLUE
**ğŸ“… Date:** 2025-10-14

---

## ğŸ“¦ Contenu du Projet (11 fichiers)

### Scripts Python (3)
1. **distill_qwen25_7b.py** - Script principal de distillation
2. **test_qwen25_7b_model.py** - Tests de validation du modÃ¨le
3. **quick_eval_qwen25_7b_1024d.py** - Ã‰valuation complÃ¨te (6 catÃ©gories)

### Scripts Shell (5)
4. **setup_qwen25_7b_env.sh** - Configuration de l'environnement
5. **run_qwen25_7b_distillation.sh** - Pipeline automatisÃ© de distillation
6. **test_qwen25_7b_model.sh** - ExÃ©cution automatique des tests
7. **evaluate_qwen25_7b.sh** - ExÃ©cution automatique de l'Ã©valuation
8. **deploy_qwen25_7b.sh** - DÃ©ploiement en production

### Documentation (3)
9. **QWEN25_7B_README.md** - Vue d'ensemble (EN)
10. **QWEN25_7B_QUICKSTART.md** - DÃ©marrage rapide (EN)
11. **QWEN25_7B_DISTILLATION_GUIDE.md** - Guide complet (EN)
12. **PRE_DISTILLATION_CHECKLIST.md** - Checklist prÃ©-distillation
13. **LISEZMOI_QWEN25_7B.md** - Ce fichier (FR)

---

## âš™ï¸ Votre Configuration MatÃ©rielle

**DÃ©tectÃ© sur votre machine:**
```
âœ… CPU:       OK (suffisant)
âš ï¸  RAM:       19GB (recommandÃ©: 32GB+)
âœ… Disque:    852GB (excellent!)
âš ï¸  GPU:       RTX 4050 5GB (recommandÃ©: 16GB+)
```

**Impact:**
- âš ï¸ GPU insuffisant â†’ Risque de Out Of Memory (OOM)
- âœ… **Solution:** Mode CPU (plus lent mais stable)
- â±ï¸ **Temps:** 10-20 heures au lieu de 2-4h

---

## ğŸš€ DÃ©marrage Rapide (Pour Votre Machine)

### Ã‰tape 1: Configuration (5 min)
```bash
./setup_qwen25_7b_env.sh
```

Ce script va:
- CrÃ©er l'environnement virtuel Python
- Installer model2vec et toutes les dÃ©pendances
- VÃ©rifier que tout fonctionne

### Ã‰tape 2: Configuration CPU Obligatoire

**âš ï¸ IMPORTANT:** Avec votre GPU 5GB, vous DEVEZ forcer le mode CPU

Ã‰ditez `distill_qwen25_7b.py` ligne 18:
```python
CONFIG = {
    "device": "cpu",  # â† Changez "cuda" en "cpu"
}
```

### Ã‰tape 3: Lancement de la Distillation (10-20h)

**Option A: Run overnight (recommandÃ©)**
```bash
screen -S distill
./run_qwen25_7b_distillation.sh
# DÃ©tacher: Ctrl+A puis D
# RÃ©attacher demain: screen -r distill
```

**Option B: Run en background**
```bash
nohup ./run_qwen25_7b_distillation.sh > distillation.log 2>&1 &
# Surveiller: tail -f distillation.log
```

### Ã‰tape 4: Validation (Le Lendemain)

**Tests (2 min):**
```bash
./test_qwen25_7b_model.sh
```

**Ã‰valuation (5 min):**
```bash
./evaluate_qwen25_7b.sh
```

**Si score â‰¥ 91%, dÃ©ploiement (10 min):**
```bash
./deploy_qwen25_7b.sh
```

---

## â±ï¸ Timeline ComplÃ¨te (Votre Machine)

```
Jour 1 - Soir:
  19:00 - Setup environnement           (5 min)
  19:05 - Ã‰diter config pour CPU        (2 min)
  19:10 - Lancer distillation           (1 min)
  19:11 - â° LAISSER TOURNER TOUTE LA NUIT

Jour 2 - Matin:
  07:00 - âœ… Distillation terminÃ©e
  07:05 - Tests du modÃ¨le               (2 min)
  07:10 - Ã‰valuation complÃ¨te            (5 min)
  07:15 - DÃ©ploiement si OK             (10 min)
  07:25 - ğŸ‰ TERMINÃ‰!
```

**Total:** ~12-15 heures (dont 10-12h distillation CPU overnight)

---

## ğŸ“Š RÃ©sultats Attendus

### MÃ©triques de QualitÃ© (Target)

| MÃ©trique | Baseline | Qwen2.5-7B | AmÃ©lioration |
|----------|----------|------------|--------------|
| **Overall** | 68.2% | **91-95%** | **+23-27%** |
| Instruction Awareness | 95.3% | 96-98% | +1-3% |
| Semantic Similarity | 95.0% | 96-98% | +1-3% |
| Code Understanding | 86.4% | 92-96% | +6-10% |
| Domain Knowledge | 65-70% | 88-92% | +18-25% |
| Multilingual | 60-65% | 85-90% | +20-28% |

### SpÃ©cifications du ModÃ¨le

```
Taille:      ~65MB (vs 14GB full = 215x plus petit)
Dimensions:  1024D
Vocabulaire: 32K tokens (Qwen tokenizer)
Vitesse:     500-1000x plus rapide
Latence:     <1ms par requÃªte
MÃ©moire:     <512MB runtime
```

---

## ğŸ¯ Pourquoi Qwen2.5-7B?

### Performance SOTA 2025

```
MMLU:      83.5%  (connaissances gÃ©nÃ©rales)
GSM8K:     93.6%  (raisonnement mathÃ©matique)
HumanEval: 89.5%  (gÃ©nÃ©ration de code)
```

### Avantages Uniques

âœ… **Meilleur modÃ¨le Qwen2.5** disponible (vs 1.5B actuel)
âœ… **Multilingue** - 29+ langues supportÃ©es
âœ… **Code-aware** - EntraÃ®nÃ© sur corpus code massif
âœ… **Instruction-tuned** - Excellent pour RAG/Q&A
âœ… **Long contexte** - 128K tokens (vs 32K pour autres)

### Avec Model2Vec

âš¡ **500-1000x plus rapide** que le modÃ¨le complet
ğŸ“¦ **215x plus petit** (65MB vs 14GB)
ğŸ’° **10-100x moins cher** en coÃ»ts compute
ğŸ”‹ **Edge-deployable** (mobile, IoT, embedded)

---

## ğŸ“‹ Checklist ComplÃ¨te

### Avant de Commencer

- [ ] Lire ce fichier en entier
- [ ] Comprendre que Ã§a va prendre 10-20h en CPU
- [ ] Avoir ~15h libres sur la machine (overnight OK)
- [ ] Fermer applications gourmandes en RAM

### Pendant la Configuration

- [ ] ExÃ©cuter: `./setup_qwen25_7b_env.sh`
- [ ] VÃ©rifier: `source venv/bin/activate` fonctionne
- [ ] Ã‰diter: `distill_qwen25_7b.py` â†’ device="cpu"
- [ ] Choisir: screen ou nohup pour run overnight

### Pendant la Distillation

- [ ] Lancer en screen/nohup (pas en foreground!)
- [ ] VÃ©rifier les premiers logs (5-10 min)
- [ ] S'assurer que Ã§a tÃ©lÃ©charge Qwen2.5-7B
- [ ] Laisser tourner overnight

### AprÃ¨s la Distillation

- [ ] VÃ©rifier que `models/qwen25-7b-deposium-1024d/` existe
- [ ] VÃ©rifier la taille: `du -sh models/qwen25-7b-deposium-1024d/`
- [ ] Lancer tests: `./test_qwen25_7b_model.sh`
- [ ] Lancer Ã©val: `./evaluate_qwen25_7b.sh`
- [ ] Si â‰¥ 91%, dÃ©ployer: `./deploy_qwen25_7b.sh`

---

## ğŸ†˜ RÃ©solution de ProblÃ¨mes

### ProblÃ¨me 1: Out Of Memory pendant distillation

**Solution:**
```bash
# VÃ©rifier que device="cpu" dans CONFIG
grep "device" distill_qwen25_7b.py

# Si toujours GPU, forcer:
export CUDA_VISIBLE_DEVICES=""
./run_qwen25_7b_distillation.sh
```

### ProblÃ¨me 2: Distillation trÃ¨s lente

**RÃ©ponse:** C'est normal en mode CPU!
- 10-20 heures attendu
- VÃ©rifier avec `htop` que Ã§a utilise bien le CPU
- Pas de panique, laissez tourner

### ProblÃ¨me 3: model2vec non installÃ©

**Solution:**
```bash
source venv/bin/activate
pip install model2vec>=0.6.0
```

### ProblÃ¨me 4: Score d'Ã©valuation < 91%

**Options:**
1. Re-distiller avec meilleur paramÃ¨tres:
   ```python
   CONFIG = {
       "pca_dims": 1536,  # Au lieu de 1024
       "corpus_size": 2_000_000,  # Au lieu de 1M
   }
   ```

2. Accepter un score < 91% si > 85%
   - Toujours meilleur que baseline (68.2%)
   - Acceptable pour production

3. Utiliser une machine plus puissante
   - Cloud GPU 16GB+
   - Score optimal avec meilleur hardware

---

## ğŸ’¡ Alternative Cloud (Si Urgent)

Si vous ne pouvez pas attendre 10-20h, utilisez le cloud:

### Option 1: AWS EC2 g5.xlarge
```
GPU:  24GB NVIDIA A10G
Prix: ~$1.00/h
Temps: 2-3 heures
CoÃ»t:  ~$2-3 total
```

### Option 2: Paperspace
```
GPU:  16GB RTX A4000
Prix: ~$0.76/h
Temps: 3-4 heures
CoÃ»t:  ~$2.50 total
```

### ProcÃ©dure Cloud

1. CrÃ©er instance avec GPU 16GB+
2. `git clone` ce repo
3. `./setup_qwen25_7b_env.sh`
4. NE PAS Ã©diter device (laissez GPU)
5. `./run_qwen25_7b_distillation.sh`
6. AprÃ¨s 2-3h, tÃ©lÃ©charger le modÃ¨le
7. DÃ©truire l'instance

---

## ğŸ“š Documentation ComplÃ¨te

### Pour DÃ©marrage Rapide
```bash
cat QWEN25_7B_QUICKSTART.md
```

### Pour RÃ©fÃ©rence ComplÃ¨te
```bash
cat QWEN25_7B_DISTILLATION_GUIDE.md
```

### Pour Checklist DÃ©taillÃ©e
```bash
cat PRE_DISTILLATION_CHECKLIST.md
```

### Pour Vue d'Ensemble Technique
```bash
cat QWEN25_7B_README.md
```

---

## ğŸ¯ CritÃ¨res de SuccÃ¨s

### âœ… PrÃªt pour Production Si:

- Overall quality â‰¥ 91%
- Instruction awareness â‰¥ 95%
- Code understanding â‰¥ 90%
- Taille modÃ¨le â‰¤ 70MB
- Tous les tests passent
- Container Docker dÃ©marre

### âš ï¸ Re-distillation NÃ©cessaire Si:

- Overall quality < 85%
- Tests Ã©chouent
- Taille modÃ¨le > 100MB
- Erreurs pendant distillation

---

## ğŸ”„ Workflow Complet

```
1. PRÃ‰PARATION (âœ… TerminÃ©)
   â”œâ”€â”€ Scripts crÃ©Ã©s
   â”œâ”€â”€ Documentation Ã©crite
   â””â”€â”€ Configuration dÃ©finie

2. SETUP (â³ Prochain, 5 min)
   â”œâ”€â”€ Environnement virtuel
   â”œâ”€â”€ Installation dÃ©pendances
   â””â”€â”€ Configuration CPU

3. DISTILLATION (â³ 10-20h overnight)
   â”œâ”€â”€ TÃ©lÃ©chargement Qwen2.5-7B
   â”œâ”€â”€ Conversion Model2Vec
   â””â”€â”€ Sauvegarde modÃ¨le

4. VALIDATION (â³ Le lendemain matin)
   â”œâ”€â”€ Tests fonctionnels
   â”œâ”€â”€ Ã‰valuation qualitÃ©
   â””â”€â”€ VÃ©rification score â‰¥ 91%

5. DÃ‰PLOIEMENT (â³ Si validÃ©)
   â”œâ”€â”€ Update API
   â”œâ”€â”€ Build Docker
   â”œâ”€â”€ Test container
   â””â”€â”€ Push production

6. DOCUMENTATION (â³ AprÃ¨s dÃ©ploiement)
   â”œâ”€â”€ Update README
   â”œâ”€â”€ Add benchmarks
   â””â”€â”€ Create summary
```

---

## ğŸ“ Support

### Documentation
- Ce fichier (FR): Vue d'ensemble et instructions
- QWEN25_7B_QUICKSTART.md: Guide rapide (EN)
- QWEN25_7B_DISTILLATION_GUIDE.md: RÃ©fÃ©rence complÃ¨te (EN)

### Ressources Externes
- Model2Vec: https://github.com/MinishLab/model2vec
- Qwen2.5: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
- HuggingFace Docs: https://huggingface.co/docs

---

## âœ… Statut Actuel

**PrÃ©paration:** âœ… ComplÃ¨te
**Configuration:** â³ Ã€ faire (5 min)
**Distillation:** â³ Ã€ lancer (10-20h)

---

## ğŸš€ DÃ‰MARRER MAINTENANT

```bash
# Ã‰tape 1: Setup (5 min)
./setup_qwen25_7b_env.sh

# Ã‰tape 2: Ã‰diter config (2 min)
nano distill_qwen25_7b.py
# Changer device: "cpu"

# Ã‰tape 3: Lancer distillation (10-20h)
screen -S distill
./run_qwen25_7b_distillation.sh
# Ctrl+A puis D pour dÃ©tacher

# Ã‰tape 4: Le lendemain (7 min)
screen -r distill  # VÃ©rifier logs
./test_qwen25_7b_model.sh
./evaluate_qwen25_7b.sh

# Ã‰tape 5: Si OK (10 min)
./deploy_qwen25_7b.sh
```

---

**DerniÃ¨re mise Ã  jour:** 2025-10-14
**PrioritÃ©:** ğŸ”¥ ABSOLUE
**Statut:** âœ… PrÃªt Ã  dÃ©marrer
**Prochaine Ã©tape:** `./setup_qwen25_7b_env.sh`
