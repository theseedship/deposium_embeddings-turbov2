# âœ… LEAF IntÃ©grÃ© avec SuccÃ¨s!

## ğŸ¯ Status: Production Ready (Local)

L'API deposium_embeddings-turbov2 intÃ¨gre maintenant **3 modÃ¨les**:

| ModÃ¨le | Dimensions | Taille | Performance | Use Case |
|--------|------------|--------|-------------|----------|
| **turbov2** | 1024D | 30MB | Ultra-rapide | Volume Ã©levÃ© |
| **int8** | 256D | 30MB | Rapide | Reranking |
| **leaf** | 768D | 441MB | 695 texts/s CPU | **PrÃ©cision max** |

## ğŸ“ API URL

**Local**: `http://localhost:11436`

## ğŸ”Œ Utilisation dans n8n

### Pour appeler LEAF dans n8n:

```json
POST http://localhost:11436/api/embed
{
  "model": "leaf",
  "input": "Votre texte ici"
}
```

### RÃ©ponse:
```json
{
  "model": "leaf",
  "embeddings": [[0.123, -0.456, ...]]  // 768 dimensions
}
```

### ModÃ¨les disponibles dans n8n:
- **"turbov2"** â†’ 1024D (ultra-rapide)
- **"int8"** â†’ 256D (compact)
- **"leaf"** â†’ 768D (prÃ©cis) â† **NOUVEAU**

## ğŸ“Š Tests RÃ©ussis

```bash
cd /home/nico/code_source/tss/deposium_embeddings-turbov2
source venv/bin/activate
python3 test_api.py
```

RÃ©sultat:
```
âœ… ALL TESTS PASSED!
- turbov2: 1 x 1024
- int8: 1 x 256
- leaf: 1 x 768 (single text)
- leaf: 3 x 768 (multiple texts)
```

## ğŸš€ DÃ©marrer l'API en Local

```bash
cd /home/nico/code_source/tss/deposium_embeddings-turbov2
source venv/bin/activate
python3 -m uvicorn src.main:app --host 0.0.0.0 --port 11436
```

L'API est maintenant accessible sur `http://localhost:11436`

## ğŸ“ Endpoints Disponibles

### 1. **GET /**
Info sur le service
```bash
curl http://localhost:11436/
```

### 2. **GET /api/tags**
Liste des modÃ¨les disponibles
```bash
curl http://localhost:11436/api/tags
```

### 3. **POST /api/embed** ou **/api/embeddings**
GÃ©nÃ©rer des embeddings
```bash
# Single text
curl -X POST http://localhost:11436/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model": "leaf", "input": "Hello world"}'

# Multiple texts
curl -X POST http://localhost:11436/api/embed \
  -H "Content-Type: application/json" \
  -d '{"model": "leaf", "input": ["Text 1", "Text 2", "Text 3"]}'
```

## ğŸŒ DÃ©ployer sur Railway

### Option 1: Push depuis local

```bash
cd /home/nico/code_source/tss/deposium_embeddings-turbov2

# 1. VÃ©rifier que tout est commitÃ©
git status

# 2. Add et commit les changements LEAF
git add .
git commit -m "Add LEAF model (768D, 441MB, 695 texts/s CPU)

- Added PyTorch 2.6.0 and transformers
- Added LEAF INT8 quantized model (768D)
- Copied LEAF model files to models/leaf_cpu/
- Updated API endpoints to support LEAF
- Performance: 695 texts/s on CPU
"

# 3. Push vers Railway
git push origin main

# Railway va automatiquement:
# - DÃ©tecter les changements
# - Rebuilder l'image Docker
# - RedÃ©marrer l'instance
# - Charger les 3 modÃ¨les (turbov2, int8, leaf)
```

### Option 2: VÃ©rifier sur Railway

1. Va sur https://railway.app/project/...
2. VÃ©rifie les logs de dÃ©ploiement
3. Une fois dÃ©ployÃ©, visite l'URL Railway (ex: https://xxx.up.railway.app/)
4. Tu devrais voir:
   ```json
   {
     "service": "Deposium Embeddings - TurboX.v2 + int8 + LEAF",
     "models": {
       "turbov2": "...",
       "int8": "...",
       "leaf": "LEAF INT8 (768D) - accurate, 695 texts/s CPU"
     }
   }
   ```

### âš ï¸ Important pour Railway

Railway va tÃ©lÃ©charger **~500MB** de dÃ©pendances (PyTorch):
- Build time: ~5-10 minutes
- Image size: ~1.5GB
- RAM needed: ~2GB

**VÃ©rifie que ton plan Railway supporte Ã§a!**

## ğŸ”§ Configuration n8n avec Railway

Une fois dÃ©ployÃ© sur Railway, change l'URL dans n8n:

```
Avant: http://localhost:11436/api/embed
AprÃ¨s:  https://xxx.up.railway.app/api/embed
```

Le reste (model name, input) reste identique!

## ğŸ“ˆ Performance Comparison

### En local (tests):
- **turbov2**: ~1000+ texts/s (Model2Vec)
- **int8**: ~500+ texts/s (Model2Vec)
- **LEAF**: **695 texts/s** (PyTorch CPU INT8)

### En production (Railway):
- Attendu: mÃªme performance ou lÃ©gÃ¨rement moins
- LEAF reste **10x plus rapide** que le target initial (20 texts/s)

## ğŸ§ª Tester avec n8n

### 1. HTTP Request Node

**URL**: `http://localhost:11436/api/embed`
**Method**: POST
**Body**:
```json
{
  "model": "leaf",
  "input": "{{ $json.text }}"
}
```

**Response**: `$json.embeddings[0]` (array de 768 nombres)

### 2. Code Node (exemple)

```javascript
// Appeler l'API LEAF
const response = await $http.request({
  method: 'POST',
  url: 'http://localhost:11436/api/embed',
  body: {
    model: 'leaf',
    input: items[0].json.text
  }
});

return {
  json: {
    text: items[0].json.text,
    embeddings: response.embeddings[0],
    dimensions: response.embeddings[0].length // 768
  }
};
```

## ğŸ’¡ Quand utiliser quel modÃ¨le dans n8n?

| ScÃ©nario | ModÃ¨le RecommandÃ© |
|----------|-------------------|
| Volume Ã©levÃ© (1000+ docs/jour) | **turbov2** (1024D) |
| Reranking / similaritÃ© rapide | **int8** (256D) |
| QualitÃ© maximale / recherche sÃ©mantique prÃ©cise | **LEAF** (768D) |
| Budget RAM limitÃ© | **int8** (256D) |
| Balance qualitÃ©/vitesse | **LEAF** (768D) âœ… |

## ğŸ› Troubleshooting

### 1. API ne dÃ©marre pas
```bash
# VÃ©rifier les logs
cd /home/nico/code_source/tss/deposium_embeddings-turbov2
source venv/bin/activate
python3 -m uvicorn src.main:app --host 0.0.0.0 --port 11436

# Si erreur "No module named...", rÃ©installer
pip install -r requirements.txt
```

### 2. LEAF ne charge pas
- VÃ©rifier que `models/leaf_cpu/` existe
- VÃ©rifier que `model_quantized.pt` est prÃ©sent (441MB)
- VÃ©rifier que PyTorch >= 2.6.0:
  ```bash
  python3 -c "import torch; print(torch.__version__)"
  ```

### 3. n8n ne peut pas se connecter
- VÃ©rifier que l'API est running: `curl http://localhost:11436/`
- VÃ©rifier le port 11436 est ouvert
- Si Railway: utiliser l'URL Railway (https://xxx.up.railway.app)

## ğŸ“Š Monitoring

### Logs de l'API

```bash
# Voir les logs en temps rÃ©el
tail -f logs/app.log  # si tu as configurÃ© le logging

# Ou via Railway
railway logs
```

### RequÃªtes dans n8n

Tu verras dans les logs de l'API:
```
INFO: Generated 1 embeddings with 768D using leaf
INFO: Generated 3 embeddings with 768D using leaf
```

## ğŸ¯ Next Steps

### Aujourd'hui (Local Testing):
1. âœ… API running en local avec LEAF
2. âœ… Tous les tests passent
3. **â†’ Tester dans n8n en local avec "leaf"**
4. **â†’ Comparer la qualitÃ© vs turbov2/int8**

### Cette semaine (Production):
1. Push vers Railway
2. Attendre le build (~5-10 min)
3. Tester l'URL Railway
4. Mettre Ã  jour n8n avec l'URL Railway
5. Monitor les performances

### Plus tard (Optimisation):
1. Comparer la qualitÃ© LEAF vs Model2Vec sur tes donnÃ©es
2. Benchmarker les coÃ»ts Railway (RAM, CPU)
3. DÃ©cider si LEAF reste ou si Model2Vec suffit

## ğŸ“ Fichiers ModifiÃ©s

```
deposium_embeddings-turbov2/
â”œâ”€â”€ requirements.txt          â† +torch 2.6.0, +transformers
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              â† +LEAF loading, +LEAF endpoint
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py      â† (nouveau)
â”‚       â””â”€â”€ student_model.py â† (copiÃ© depuis deposium_training_LEAF)
â”œâ”€â”€ models/leaf_cpu/         â† (nouveau)
â”‚   â”œâ”€â”€ model_quantized.pt   â† 441MB
â”‚   â”œâ”€â”€ tokenizer.json       â† 33MB
â”‚   â””â”€â”€ config.json
â”œâ”€â”€ test_api.py              â† (nouveau) Script de test
â””â”€â”€ LEAF_INTEGRATION_DONE.md â† Ce fichier
```

## ğŸ‰ Conclusion

Tu as maintenant:
- âœ… API multi-modÃ¨les (turbov2, int8, **LEAF**)
- âœ… LEAF optimisÃ© CPU (695 texts/s)
- âœ… PrÃªt pour n8n (model name: **"leaf"**)
- âœ… PrÃªt pour Railway (push quand tu veux)

**Le modÃ¨le LEAF est intÃ©grÃ© et prÃªt Ã  l'emploi! ğŸš€**

---

**Questions frÃ©quentes**:
- **Nom du modÃ¨le dans n8n**: `"leaf"`
- **Dimensions**: 768
- **Performance**: 695 texts/s sur CPU
- **Taille**: 441MB
- **RedÃ©marrer instance Railway?**: Oui, aprÃ¨s le push (automatique)
